

<br>

ì˜ˆì¸¡ë¶„ì„ì˜ ëª©í‘œ : íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ëª¨ë¸ì„ ë§Œë“¤ê³ , ë§Œë“¤ì–´ì§„ ëª¨ë¸ë¡œ ìƒˆë¡œìš´ ê°’(ìƒˆë¡œìš´ ë…ë¦½ë³€ìˆ˜)ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ê²ƒ.

<br>

# **íšŒê·€ë¶„ì„**  

í•˜ë‚˜ì˜ ë…ë¦½ë³€ìˆ˜ê°€ ì¢…ì†ë³€ìˆ˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì§€ë¥¼ íŒŒì•…í•´ ì¸ê³¼ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ë¶„ì„ë°©ë²•ì´ë‹¤.  
ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì•Œì•„ë³´ê³  ë…ë¦½ë³€ìˆ˜ì˜ ê°’ìœ¼ë¡œ ì¢…ì†ë³€ìˆ˜ì˜ ê°’ì„ ì˜ˆì¸¡í•  ìˆ˜ë„ ìˆë‹¤.  

íšŒê·€ì˜ ê¸°ë³¸ ì›ë¦¬ëŠ” ì„ í˜•íšŒê·€ëª¨ë¸ì˜ ì§ì„ ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ ì°¨ì´ì¸ ì”ì°¨(error, cost, lost)ë¥¼ ìµœì†Œí™” ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.
ì¦‰, ìµœì ì˜ ì¶”ì„¸ì„ ì´ë€ ì”ì°¨ì œê³±í•©ì´ ìµœì†Œê°€ ë˜ëŠ” ì„ ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.
ìµœì ì˜ ì¶”ì„¸ì„ ì„ êµ¬í•´ ì˜ˆì¸¡ê³¼ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•œë‹¤.  

ë…ë¦½ë³€ìˆ˜ Xë¡œ ì¢…ì†ë³€ìˆ˜ Yë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ì¶”ì„¸ì„ (ì¼ì°¨ë°©ì •ì‹)
> y = wx + b

w : ê¸°ìš¸ê¸°(weight(ê°€ì¤‘ì¹˜))  
b : ì ˆí¸(bias(í¸í–¥))


costê°€ ìµœì†Œì¸ ì¶”ì„¸ì„ ì„ êµ¬í•˜ëŠ” ê³¼ì •ì„ í•™ìŠµ(train)ì´ë¼ê³  í•˜ë©° í•™ìŠµì—ì„œ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ê²½ì‚¬í•˜ê°•ë²•ì´ë‹¤.

<br>

## ìµœì†Œì œê³±ë²•  

ìµœì†Œì œê³±ë²•ì„ ì´ìš©í•´ ì¼ì°¨ë°©ì •ì‹ w(ê¸°ìš¸ê¸°)ì™€ b(ì ˆí¸)


```py
import numpy as np
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic')
plt.rcParams['axes.unicode_minus'] = False

x = np.array([0,1,2,3])
y = np.array([-1,0.3,0.9,2.2])

plt.plot(x,y)
plt.grid(True)
plt.show()

A = np.vstack([x, np.ones(len(x))]).T  #x ì¸ìˆ˜ë¥¼ í–‰ë ¬ë¡œ ë°›ê¸° ë•Œë¬¸ì— í–‰ë ¬ë¡œ êµ¬ì„±
print(A)
print()

#ìµœì†Œì œê³±í•©ì„ êµ¬í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy.linalg as lin  

print(lin.lstsq(A, y))
print()

w, b = lin.lstsq(A, y)[0]
print(w, b)
print()
#y = wx + b

plt.plot(x, y, 'o', label = 'ì‹¤ì œê°’', markersize = 10)
plt.plot(x, w * x + b, 'r', label = 'ìµœì í™”ëœ ì¶”ì„¸ì„ ')
plt.legend()
plt.show()

yhat = w * 2 + b
print(yhat)
```

<br>

# **ì„ í˜•íšŒê·€ Linear Regression**

ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ì™€ì˜ ì„ í˜• ìƒê´€ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì´ë‹¤. 
ë‘ ë³€ìˆ˜ëŠ” ìƒê´€ê´€ê³„ê°€ ìˆì–´ì•¼ í•˜ë©° ì¸ê³¼ê´€ê³„ê°€ ì„±ë¦½í•´ì•¼ í•œë‹¤. 

ë…ë¦½ë³€ìˆ˜ : ì—°ì†í˜•  
ì¢…ì†ë³€ìˆ˜ : ì—°ì†í˜•  

ê¸°ë³¸ ì¶©ì¡± ì¡°ê±´ : ì„ í˜•ì„±, ì”ì°¨ì •ê·œì„±, ë…ë¦½ì„±, ë“±ë¶„ì‚°ì„±, ë‹¤ì¤‘ê³µì„ ì„±

<br>

# **ë‹¨ìˆœì„ í˜•íšŒê·€ ë¶„ì„ (Simple Linear Regression)**

<br>

### ë°ì´í„° ìƒì„± : `make_regression()`

`make_regression()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒ˜í”Œì„ ìƒì„±í•œë‹¤.  
Scikit-learnì—ì„œ featureì˜ í˜•íƒœë¥¼ 2ì°¨ì› ë°ì´í„°(í–‰ë ¬)ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— feature ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í˜•í•´ì£¼ì–´ì•¼ í•œë‹¤.

```py
import statsmodels.api as sm
from sklearn.datasets import make_regression
import numpy as np
import pandas as pd

np.random.seed(12)

x, y, coef = make_regression(n_samples=50, n_features=1, bias=100.0, coef = True)

print(x)    #sklearnì—ì„œ xëŠ” matrix
print(y)    #yëŠ” vector í˜•ì‹
print(coef) #ê°€ì¤‘ì¹˜ w
print()

y_pred = 89.47430739278907 * 0.64076111 + 100
print('ì˜ˆì¸¡ê°’:', y_pred)
print()

y_pred = 89.47430739278907 * -1.70073563 + 100
print('ì˜ˆì¸¡ê°’:', y_pred)
print()

new_x = 1.23
y_pred_new = 89.47430739278907 * new_x + 100
print('ì˜ˆì¸¡ê°’:', y_pred_new)
```

<br>

## ëª¨ë¸ ìƒì„±

<br>

### ë°©ë²• 1 : Scikit-learnì˜ `LinearRegression()` í•¨ìˆ˜ ì‚¬ìš©

```py
xx = x
yy = y

from sklearn.linear_model import LinearRegression

#ëª¨ë¸ ê°ì²´ ìƒì„±
model = LinearRegression()
print(model)

#í•™ìŠµ
fit_model = model.fit(xx, yy)
print(fit_model.coef_)
print(fit_model.intercept_)

#ì˜ˆì¸¡ê°’ í™•ì¸
y_pred = fit_model.predict(xx[[0]])
print('ì˜ˆì¸¡ê°’:', y_pred)

#ìƒˆë¡œìš´ ê°’ìœ¼ë¡œ í™•ì¸
new_x = 1.23
y_pred_new = fit_model.predict([[new_x]])  #matrixë¡œ ì…ë ¥
print('ì˜ˆì¸¡ê°’:', y_pred_new)
```

<br>

### ë°©ë²• 2 : `ols()` ì‚¬ìš©   

Scikit-learnì—ì„œëŠ” í–‰ë ¬ê°’ì„ ì‚¬ìš©í–ˆìœ¼ë‚˜, olsì—ì„œëŠ” 1ì°¨ì› ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤.  
ì•ì„œ ì‚¬ìš©í•˜ë˜ 2ì°¨ì›ìœ¼ë¡œ ëœ ë°ì´í„°ë¥¼ ì°¨ì›ì¶•ì†Œ í•´ì£¼ì–´ì•¼ í•œë‹¤.  


```py
import statsmodels.formula.api as smf

print(xx.shape) #2ì°¨ì›  

x1 = xx.flatten()
print(x1.shape)
print(x1)

y1 = yy
print(y1)

data = np.array([x1, y1])
df = pd.DataFrame(data.T)
print(df.head(3))

model2 = smf.ols(formula = 'y1 ~ x1', data=df).fit()
print(model2.summary())
print()

#ì˜ˆì¸¡
print(x1[:2])
print(y1[:2])
print()

new_df = pd.DataFrame({'x1':[-1.70073563, -0.67794537]})  #DataFrameìœ¼ë¡œ í•™ìŠµì‹œì¼°ìœ¼ë¯€ë¡œ DFí˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì…ë ¥  
new_pred = model2.predict(new_df)
print('ì˜ˆì¸¡ê°’:', new_pred)
print()

new2_df = pd.DataFrame({'x1':[1.23, -3.21]}) 
new2_pred = model2.predict(new2_df)
print('ì˜ˆì¸¡ê°’:', new2_pred)
```

<br>

### ë°©ë²• 3 : Scipy statsì˜ `linregress()` í•¨ìˆ˜ ì‚¬ìš©  


```py
from scipy import stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

score_iq = pd.read_csv('/content/drive/MyDrive/testdata/score_iq.csv')
print(score_iq.head(3))
print(score_iq.describe())
print(score_iq.corr())

#iqê°€ scoreì— ì˜í–¥ì„ ì¤€ë‹¤ê³  ê°€ì •í•œë‹¤.
x = score_iq.iq
y = score_iq.score

#ë‘ ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ í™•ì¸
print(np.corrcoef(x,y))
print()

#ê·¸ë˜í”„
plt.scatter(x, y)
plt.xlabel('iq')
plt.ylabel('score')
plt.show()

model = stats.linregress(x,y)

print(model)
print('ê¸°ìš¸ê¸°:', model.slope)
print('ì ˆí¸:', model.intercept)
print('ì„¤ëª…ë ¥:', model.rvalue**2)
print('p-value:', model.pvalue)
print()
```

pvalue=2.8476895206683644e-50ë¡œ ì¸ê³¼ê´€ê³„ê°€ ì¡´ì¬í•œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. 
íšŒê·€ëª¨ë¸ì€ ì í•©í•˜ë‹¤. 

```py
#ì˜ˆì¸¡
new_x = 125
ypred = model.slope * new_x + model.intercept
print('ì˜ˆì¸¡ê°’:', ypred)
```
`predict()` í•¨ìˆ˜ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤. ëŒ€ì‹  Numpyì˜ `polyval()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.



```py
#ì „ì²´ê°’ ì˜ˆì¸¡
print('í•¨ìˆ˜ì˜ˆì¸¡:', np.polyval([model.slope, model.intercept], np.array(score_iq['iq'])))
print()

#ìƒˆë¡œìš´ ê°’ ì˜ˆì¸¡
new_df = pd.DataFrame({'iq':[55,66,77,88,155]})
print('í•¨ìˆ˜ì˜ˆì¸¡:', np.polyval([model.slope, model.intercept], new_df))
```

<br>

## ğŸ‘‰ **`ols()` ê²°ê³¼ í•´ì„í•˜ê¸°** 


```py
import pandas as pd
import statsmodels.formula.api as smf

df = pd.read_csv('/content/drive/MyDrive/testdata/drinking_water.csv')
print(df.head(3))
print(df.corr())
print()

model = smf.ols(formula='ë§Œì¡±ë„ ~ ì ì ˆì„±', data=df).fit()
print(model.summary())
```

> Prob (F-statistic):  2.24e-52  

Fê°’ì— ì˜í•´ ë§Œë“¤ì–´ì§„ p-valueì´ë‹¤. ëª¨í˜•ì˜ ì í•©ë„ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤.  
p-value < 0.05 ì´ë©´ í•´ë‹¹ íšŒê·€ëª¨ë¸ì„ ìœ ì˜í•˜ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.  (ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ì‚¬ì´ì— ì¸ê³¼ê´€ê³„ê°€ ì¡´ì¬í•œë‹¤.)

> Intercept  0.7789 -> ì ˆí¸   
ì ì ˆì„±     0.7393 -> ê¸°ìš¸ê¸°, Î²  

ê¸°ìš¸ê¸°ì˜ ê°ë„ê°€ ì–¼ë§ˆì¸ì§€ëŠ” ìƒê´€ì´ ì—†ë‹¤. ë‹¤ë§Œ ê¸°ìš¸ê¸°ê°€ 0ì´ì–´ì„œëŠ” ì•ˆëœë‹¤.  0ì´ë©´ ë…ë¦½ë³€ìˆ˜ê°€ ì•„ë¬´ë¦¬ ë³€í™”í•´ë„ ì¢…ì†ë³€ìˆ˜ê°€ ë³€í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.  

ê°€ì„¤  
H0 : ê¸°ìš¸ê¸°(Î²) = 0  
H1 : ê¸°ìš¸ê¸°(Î²) != 0  

í‘œë³¸ì˜¤ì°¨ t, pë¡œ ê¸°ìš¸ê¸° ê°’ì´ ìœ ì˜í•œì§€ íŒë‹¨í•œë‹¤.

> std err  0.038

í‘œì¤€ì˜¤ì°¨ : í‘œë³¸í‰ê· ë“¤ì˜ í‘œì¤€í¸ì°¨
ëª¨ì§‘ë‹¨ì´ë‘ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ” ê°€ì— ëŒ€í•œ ê°’ì´ë‹¤. ì¦‰, ëª¨ì§‘ë‹¨ í‰ê· ê³¼ í‘œë³¸í‰ê· ê³¼ì˜ ì°¨ì´ì´ë‹¤.  

í‘œì¤€ì˜¤ì°¨ê°€ ì‘ì„ ìˆ˜ë¡ ëª¨ì§‘ë‹¨ì— ìœ ì‚¬í•˜ë‹¤.   

í‘œë³¸ì˜¤ì°¨ t, pë¡œ ê¸°ìš¸ê¸° ê°’ì´ ìœ ì˜í•œì§€ íŒë‹¨í•  ìˆ˜ ìˆë‹¤.

> t  19.340  

tê°’  = ê¸°ìš¸ê¸° / í‘œì¤€ì˜¤ì°¨    
19.340 = 0.7393 /  0.038   

tì˜ ì œê³± = F-statistic: (Fê°’)     
19.340**2 = 374.0  

tê°’ì„ í†µí•´ì„œ Fê°’ ì‚°ì¶œë˜ë©° Fê°’ í†µí•´ì„œ p-value ì‚°ì¶œëœë‹¤.  
tê°’ê³¼ pê°’ì€ ë°˜ë¹„ë¡€ê´€ê³„ì´ë‹¤.

> P>|t|   0.000  

ê°ê°ì˜ ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ pê°’ì´ë‹¤.  
ë…ë¦½ë³€ìˆ˜ì˜ ì ì ˆì„±, ê°ê°ì˜ ë…ë¦½ë³€ìˆ˜ê°€ ìœ ì˜í•œì§€ íŒë‹¨í•˜ëŠ” ê°’ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.    
p < 0.05ì´ë¯€ë¡œ í•´ë‹¹ ë…ë¦½ë³€ìˆ˜ê°€ ìœ ì˜í•˜ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.

> [0.025   &emsp;   0.975]  

0.664   0.815  => ì‹ ë¢°êµ¬ê°„ì— í•´ë‹¹í•˜ëŠ” ê°’ì´ë‹¤.

> R-squared:   0.588     

ê²°ì •ê³„ìˆ˜, ëª¨ë¸ì˜ ì„¤ëª…ë ¥.  
ë…ë¦½ë³€ìˆ˜ê°€ ì¢…ì†ë³€ìˆ˜ì˜ ë¶„ì‚°ì„ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ”ê°€ì— ëŒ€í•œ ê°’ì´ë‹¤.     
ê²°ì •ê³„ìˆ˜ëŠ” 0ì—ì„œ 1ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë©° ë†’ì„ ìˆ˜ë¡ ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì´ ë†’ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.  
0.588ì´ë©´ ê½¤ ë†’ì€í¸ì´ë‹¤.  
í˜„ì¥ì—ì„œëŠ” 0.20 ~ 0.15 (20% ~ 15%) ì •ë„ë©´ ì„¤ëª…ë ¥ì´ ìˆë‹¤ê³  ë³¸ë‹¤.  


íšŒê·€ì§ì„ ì˜ ì í•©ë„(goodness-of-fit)ë¥¼ í‰ê°€í•˜ê±°ë‚˜ ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ë…ë¦½ë³€ìˆ˜ë“¤ì˜ ì„¤ëª…ë ¥ì„ ì•Œê³ ì í•  ë•Œ ì´ìš©í•œë‹¤.  
ì„¤ëª…ëœ ë¶„ì‚°ì€ ì¢…ì†ë³€ìˆ˜ì˜ ë¶„ì‚°ê³¼ ë…ë¦½ë³€ìˆ˜ê°€ ë‚˜íƒ€ë‚´ëŠ” ë¶„ì‚°ì˜ êµì§‘í•©ì´ë‹¤.  êµì§‘í•©ì´ ì•„ë‹Œë¶€ë¶„ì€ ì”ì°¨(error)ì˜ ë¶„ì‚°ì´ë‹¤.  
ì„¤ëª…ë ¥ì´ ë†’ì„ ìˆ˜ë¡ êµì§‘í•©ì´ ì»¤ì§€ë©° ë…ë¦½ë³€ìˆ˜ê°€ ì¢…ì†ë³€ìˆ˜ì˜ ë§ì€ ë¶€ë¶„ì„ ì„¤ëª…í•œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.  


ê²°ì •ê³„ìˆ˜ = ì„¤ëª…ëœ ë¶„ì‚°ê°’ / ì¢…ì†ë³€ìˆ˜ì˜ ì „ì²´ë¶„ì‚°  

ìƒê´€ê³„ìˆ˜ Rì„ ì œê³±í•˜ê±°ë‚˜ í˜¹ì€ 1 - SSE / SST ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤.

SSE = ëª¨ë“  ì˜¤ì°¨ì—ì„œ ì¶”ì •ì¹˜ë¥¼ ëº€ ê°’ ì œê³±í•©.  
SSR = ì¶”ì •ì¹˜ì—ì„œ í‰ê· ì„ ëº€ ê°’ ì œê³±í•©.  
SST = ì˜¤ì°¨ì—ì„œ í‰ê· ì„ ëº€ê°’ ì œê³±í•©.  SSE + SSR  

 

ë…ë¦½ë³€ìˆ˜ê°€ í•˜ë‚˜ì¼ë•ŒëŠ” ê²°ì •ê³„ìˆ˜ë¡œ í™•í•˜ê³  ë…ë¦½ë³€ìˆ˜ê°€ ë‘ ê°œ ì´ìƒì¼ ë•ŒëŠ” ìˆ˜ì •ëœ ê²°ì •ê³„ìˆ˜(Adj. R-squared:  0.586)ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒë‹¨í•œë‹¤.    

ê²°ì •ê³„ìˆ˜ëŠ” ë…ë¦½ë³€ìˆ˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ê·¸ ê°’ì´ ì»¤ì§€ê²Œ ëœë‹¤. ë”°ë¼ì„œ ì¢…ì†ë³€ìˆ˜ì˜ ë¶„ì‚°ì„ ì„¤ëª…í•´ ì£¼ì§€ ëª»í•˜ëŠ” ë³€ìˆ˜ê°€ ëª¨í˜•ì— ì¶”ê°€ëœë‹¤ê³  í•˜ë”ë¼ë„ ê²°ì •ê³„ìˆ˜ê°’ì´ ì»¤ì§ˆ ìˆ˜ ìˆë‹¤.  
ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë³´ì •í•œ ê²ƒì´ ìˆ˜ì •ëœ ê²°ì •ê³„ìˆ˜ì´ë‹¤. ìˆ˜ì •ê²°ì •ê³„ìˆ˜ëŠ” í‘œë³¸ì˜ í¬ê¸°ì™€ ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ê³„ì‚°í•œë‹¤.  

í‘œë³¸ì˜ í¬ê¸°ê°€ 200ê°œ ì´ìƒì¼ ë•ŒëŠ” ë‘ ê²°ì •ê³„ìˆ˜ì˜ ì°¨ì´ê°€ ë¯¸ë¯¸í•˜ê¸° ë•Œë¬¸ì— í‘œë³¸ì´ 200ê°œ ë¯¸ë§Œì¼ ë•ŒëŠ” ë°˜ë“œì‹œ ìˆ˜ì •ê²°ì •ê³„ìˆ˜ë¥¼ ê³ ë ¤í•´ì•¼ í•¨.  



> Durbin-Watson:  2.185   

ì”ì°¨ì˜ ë…ë¦½ì„± ê²€ì •.  ìê¸° ìƒê´€ê´€ê³„ë¥¼ íŒë‹¨í•œë‹¤.  
0~4ê°’ì„ ê°€ì§€ë©°  2ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ë…ë¦½ì„±ì„ ì„±ë¦½í•œë‹¤ê³  ë³¸ë‹¤.  ì¦‰, 2ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìê¸°ìƒê´€ì´ ì—†ë‹¤.  

ì£¼ë¡œ ì‹œê³„ì—´ ë°ì´í„° ë“±ì—ì„œ íŒ¨í„´ì´ ë°˜ë³µë˜ëŠ” í˜„ìƒ -> ìê¸°ìƒê´€ì´ ë°œìƒí•œë‹¤.

> Skew:   -0.328  

ì™œë„.  
ìŒìˆ˜ë©´ ì™¼ìª½ìœ¼ë¡œ ê¼¬ë¦¬ê¸´ ë¶„í¬ëª¨ì–‘, ì–‘ìˆ˜ë©´ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¼¬ë¦¬ê°€ ê¸´ ë¶„í¬ëª¨ì–‘ì„ ê°€ì§„ë‹¤.

> Kurtosis: 4.012  

ì²¨ë„.
ìŒìˆ˜ë©´ ë¶„í¬ê°€ ì™„ë§Œ, ì–‘ìˆ˜ë©´ ë¶„í¬ê°€ ë¾°ì¡±í•œ ëª¨ì–‘ì´ë‹¤.


```py
print('coef:\n', model.params)
print('\nr-squared:\n', model.rsquared)
print('\np-value:\n', model.pvalues)
#print('\npred values:\n', model.predict())
print('\npred values:\n', df.ì ì ˆì„±[0], model.predict()[0])

#ìƒˆë¡œìš´ ê°’ ì˜ˆì¸¡
print(df.ì ì ˆì„±[:5].values)

new_df = pd.DataFrame({'ì ì ˆì„±':[4, 3, 4, 2, 7]})
new_pred = model.predict(new_df)
print(new_pred)

#ì‹œê°í™”
import numpy as np
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic')
plt.rcParams['axes.unicode_minus'] = False

plt.scatter(df.ì ì ˆì„±, df.ë§Œì¡±ë„)
slope, intercept = np.polyfit(df.ì ì ˆì„±, df.ë§Œì¡±ë„, 1)
plt.plot(df.ì ì ˆì„±, df.ì ì ˆì„± * slope + intercept, 'b')
plt.show()
```

<br>

# ğŸŒ· iris datasetìœ¼ë¡œ íšŒê·€ë¶„ì„ : `ols()` 

`ols` ìƒê´€ê´€ê³„ì˜ ê°•ë„ì— ë”°ë¼ íšŒê·€ë¶„ì„ ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ íŒë‹¨í•œë‹¤.  


```py
import pandas as pd
import seaborn as sns
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt

iris = sns.load_dataset('iris')
print(iris.head(3))

sns.pairplot(iris, hue='species', height=1.5)
plt.show()
```

<br>

## ìƒê´€ê´€ê³„ í™•ì¸  

```py
print(iris.corr())
```

<br>

## ìƒê´€ê´€ê³„ê°€ ì•½í•œ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì‘ì„±  


```py
result1 = smf.ols(formula='sepal_length ~ sepal_width', data = iris).fit()
print(result1.summary())

print('\nì„¤ëª…ë ¥:', result1.rsquared)
print('p-value:', result1.pvalues)
```

ê²°ì •ê³„ìˆ˜ê°€ R-squared: 0.014 ë¡œ ì„¤ëª…ë ¥ì´ ë§¤ìš° ì•½í•˜ë‹¤.  
ë˜í•œ ë…ë¦½ë³€ìˆ˜ sepal_widthì— ëŒ€í•œ p-valueê°€ 0.152ë¡œ 0.05ë³´ë‹¤ í¬ë¯€ë¡œ ì¢…ì†ë³€ìˆ˜ì— ì˜í–¥ì„ ì¤€ë‹¤ê³  ë³´ê¸° ì–´ë ¤ì›Œ ë…ë¦½ë³€ìˆ˜ì— ë¶€ì í•©í•˜ë‹¤.  
ê·¸ëŸ¬ë¯€ë¡œ ì´ ëª¨ë¸ì€ ì“¸ë§Œí•œ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ì—†ë‹¤.

<br>

## ìƒê´€ê´€ê³„ê°€ ê°•í•œ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì‘ì„±

```py
result2 = smf.ols(formula='sepal_length ~ petal_length', data = iris).fit()
print(result2.summary())

print('\nì„¤ëª…ë ¥:', result2.rsquared)
print('p-value:', result2.pvalues)
```

ê²°ì •ê³„ìˆ˜ë¡œ R-squared: 0.760 ì„¤ëª…ë ¥ì´ ë†’ìœ¼ë©°  
p-valueê°€ 1.038667e-47 < 0.05 ì´ë¯€ë¡œ petal_lengthëŠ” ë…ë¦½ë³€ìˆ˜ë¡œ ì í•©í•˜ë‹¤.  
ë˜í•œ ì´ ëª¨ë¸ì€ ì¢‹ì€ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.  


ì¦‰, ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ê°€ ê°•í•´ì•¼ ì¢‹ì€ ëª¨ë¸ì´ ë§Œë“¤ì–´ì§ˆ ìˆ˜ ìˆë‹¤.

<br>

## ì˜ˆì¸¡ê°’ í™•ì¸


```py
#ê¸°ì¡´ë°ì´í„°
print(iris['petal_length'][:5])
print('ì‹¤ì œê°’:', iris['sepal_length'][:5])
print('ì˜ˆì¸¡ê°’:', result2.predict(iris['petal_length'][:5]))

#ìƒˆë¡œìš´ ë°ì´í„°
new_data = pd.DataFrame({'petal_length':[1.0,1.8,3.8]})
y_pred = result2.predict(new_data)
print('ì˜ˆì¸¡ê°’:\n', y_pred)
```

<br>

## ì°¸ê³  : ìˆ˜ì‹ë§Œë“¤ê¸°  

Rì—ì„œì˜ formula = 'sepal_length ~.' ë°©ë²•ì€ ì‚¬ìš© ë¶ˆê°€í•˜ë‹¤. ë§ì€ ë…ë¦½ë³€ìˆ˜ë¥¼ í•œë²ˆì— ì…ë ¥í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì  


```py
column_select = ' + '.join(iris.columns.difference(['sepal_length','species']))
print(column_select)

my_formula = 'sepal_length ~ ' + column_select
print(my_formula)

result3 = smf.ols(formula = my_formula , data = iris).fit()
print(result3.summary())
```

<br>

# **ë‹¤ì¤‘ì„ í˜•íšŒê·€ (Multiple Linear Regression)**  

íšŒê·€ë¶„ì„ì—ì„œ ë…ë¦½ë³€ìˆ˜ê°€ 2ê°œ ì´ìƒì¼ ê²½ìš° ë‹¤ì¤‘íšŒê·€ë¶„ì„ì´ë¼ê³  í•œë‹¤.   

```py
import pandas as pd
import statsmodels.formula.api as smf

df = pd.read_csv('/content/drive/MyDrive/testdata/drinking_water.csv')

model2 = smf.ols('ë§Œì¡±ë„ ~ ì ì ˆì„± + ì¹œë°€ë„', data=df).fit()
print(model2.summary())
```

<br>

# ğŸš— mtcars datasetìœ¼ë¡œ ì„ í˜•íšŒê·€ ë¶„ì„ : `ols()`

```py
import statsmodels.api
import statsmodels.formula.api as smf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic')
plt.rcParams['axes.unicode_minus'] = False

mtcars = statsmodels.api.datasets.get_rdataset('mtcars').data
print(mtcars.head(3))
print(mtcars.columns)
print()

print(mtcars.describe())
print(mtcars.info())

print(np.corrcoef(mtcars.hp, mtcars.mpg))  #ë§ˆë ¥ìˆ˜, ì—°ë¹„
print(np.corrcoef(mtcars.wt, mtcars.mpg))  #ì°¨ì²´ë¬´ê²Œ, ì—°ë¹„
```
<br>

## ì‹œê°í™” 

```py
plt.scatter(mtcars.hp, mtcars.mpg)
plt.xlabel('ë§ˆë ¥ìˆ˜')
plt.ylabel('ì—°ë¹„')

#ì¶”ì„¸ì„ ê·¸ë¦¬ê¸°
slope, intercept = np.polyfit(mtcars.hp, mtcars.mpg, 1)
plt.plot(mtcars.hp, mtcars.hp * slope + intercept, 'r')
plt.show()
```

<br>

## ë‹¨ìˆœì„ í˜•íšŒê·€ ëª¨ë¸  


```py
result = smf.ols('mpg ~ hp', data=mtcars).fit()
print(result.summary())
print()

#ì‹ ë¢°êµ¬ê°„
print(result.conf_int())
print(result.conf_int(alpha=0.05))
print(result.summary().tables[1])

print(-0.0682 * 110 + 30.0989)
print(-0.0682 * 50 + 30.0989)
print(-0.0682 * 250 + 30.0989)
```

ì„¤ëª…ë ¥ì´ 0.602ì´ê³ 
hpì— ëŒ€í•œ p-valueê°€ 0.000 > 0.05 ì´ë¯€ë¡œ hpëŠ” ì í•©í•œ ë…ë¦½ë³€ìˆ˜ë¼ê³  í•  ìˆ˜ ìˆë‹¤.  


```py
#ì¶”ì •ì¹˜ êµ¬í•˜ê¸° - ì°¨ì²´ ë¬´ê²Œë¥¼ ì…ë ¥í•´ ì—°ë¹„ë¥¼ ì¶”ì •
result3 = smf.ols('mpg ~ wt', data=mtcars).fit()
print(result3.summary().tables[1])
print('ê²°ì •ê³„ìˆ˜:', result3.rsquared)

pred = result3.predict()
#print(pred)
print(mtcars.mpg[0])  #ì‹¤ì œê°’
print(pred[0])  #ì˜ˆì¸¡ê°’
print()

#DataFrameìœ¼ë¡œ ë³´ê¸°
data = {
    'mpg':mtcars.mpg,
    'mpg_pred':pred
}
df = pd.DataFrame(data)
print(df)
```
<br>

```py
#ìƒˆë¡œìš´ ì°¨ì²´ ë¬´ê²Œì— ëŒ€í•œ ì—°ë¹„ ì¶”ì •

mtcars.wt = float(input('ì°¨ì²´ ë¬´ê²Œ ì…ë ¥:'))
new_pred = result3.predict(pd.DataFrame(mtcars.wt))

print('ì°¨ì²´ë¬´ê²Œ: {}ì¼ ë•Œ ì˜ˆìƒì—°ë¹„ëŠ” {}'.format(mtcars.wt[0], new_pred[0]))

new_wts = pd.DataFrame({'wt':[6,3,0.5]})
new_pred2 = result3.predict(new_wts)

print('ì˜ˆìƒì—°ë¹„:\n', new_pred2)
print('\nì˜ˆìƒì—°ë¹„:\n', np.round(new_pred2.values, 2))
```

<br>

## ë‹¤ì¤‘íšŒê·€ ëª¨ë¸ 

```py
result2 = smf.ols('mpg ~ hp + wt', data=mtcars).fit()
print(result2.summary())
print()

print(result.conf_int(alpha=0.05))
print((-0.0318 * 110) + (-3.8778 * 5) + 37.2273)
```

<br>

# Kaggleì˜ Advertising dataset : `ols()`  


```py
import statsmodels.formula.api as smf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

adv = pd.read_csv('/content/drive/MyDrive/testdata/Advertising.csv', usecols=[1,2,3,4])
print(adv.head(5))

print(adv.index, adv.columns)
print(adv.info())
print()

#ìƒê´€ê³„ìˆ˜ í™•ì¸
print(adv.corr())
print(adv.loc[:,['sales','tv']].corr())
print()

#ëª¨ë¸ ì‘ì„±
#lm = smf.ols(formula='sales ~ tv', data=adv)
#lm_learn = lm.fit()

lm = smf.ols(formula='sales ~ tv', data=adv).fit()
print(lm.summary())
print()

print(lm.params)
print(lm.pvalues)
print(lm.rsquared)
```

<br>

## ì‹œê°í™”


```py
import seaborn as sns

plt.scatter(adv.tv, adv.sales)
plt.xlabel('tv')
plt.ylabel('sales')

x = pd.DataFrame({'tv':[adv.tv.min(), adv.tv.max()]})
y_pred = lm.predict(x)
plt.plot(x, y_pred, c='red')

plt.title('Linear Regression')
plt.xlim(-50, 400)
plt.ylim(ymin=0)
plt.show()
```

<br>

## ì˜ˆì¸¡ 1 : ìƒˆë¡œìš´ tv ê°’ìœ¼ë¡œ sales ì¶”ì • : ë‹¨ìˆœíšŒê·€  


```py
x_new = pd.DataFrame({'tv':[500, 50, 1000]})
pred = lm.predict(x_new)

print('ì˜ˆìƒ ë§¤ì¶œ:\n', pred.values)
```

<br>

## ì˜ˆì¸¡ 2 : ë‹¤ì¤‘íšŒê·€

### ëª¨ë¸ì‘ì„±

```py
#newspaper í¬í•¨
lm = smf.ols(formula='sales ~ tv + radio + newspaper', data=adv).fit()
print(lm.summary())

#newspaper ì œì™¸
lm_mul = smf.ols(formula='sales ~ tv + radio', data=adv).fit()
print(lm_mul.summary())
```

ìê¸°ìƒê´€ê³„ìˆ˜ Durbin-WatsonëŠ” ëª¨ë‘ 2ì ëŒ€ë¡œ ì•ˆì •ì   

newspaper ë³€ìˆ˜ í¬í•¨ ì‹œ Adj. R-squared: 0.896  
newspaper ë³€ìˆ˜ ì œì™¸ ì‹œ Adj. R-squared: 0.896  

newspaper ë³€ìˆ˜ì˜ p-value 0.860 > 0.05  

ìƒê´€ê´€ê³„ê°€ ë‚®ì•˜ë˜ ë³€ìˆ˜ì¸ newspaperì˜ ì œì™¸ ì „, í›„ ëª¨ë¸ì˜ ê²°ì •ê³„ìˆ˜ì— ì°¨ì´ê°€ ì—†ë‹¤.  
ì¦‰, newspaper ë³€ìˆ˜ëŠ” ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì— ì˜í–¥ì„ ë¼ì¹˜ì§€ ì•ŠëŠ”ë‹¤. ì œì™¸í•˜ê³  ëª¨ë¸ì„ ë§Œë“¤ì–´ë„ ë¬´ë°©í•˜ë‹¤.

<br>

ìƒˆë¡œìš´ tv, radio ê°’ìœ¼ë¡œ sales ì˜ˆì¸¡í•˜ê¸°

```py
x_new2 = pd.DataFrame({'tv':[200, 55.5, 100], 'radio':[30.1, 45.5, 50.1]})
pred2 = lm_mul.predict(x_new2)

print('ì¶”ì •ê°’:', pred2.values)
```

<br>

# ğŸŒ íšŒê·€ëª¨ë¸ì˜ ì ì ˆì„±ì„ íŒë‹¨í•˜ëŠ” ê¸°ì¤€  

>1. ì •ê·œì„± : ë…ë¦½ë³€ìˆ˜ë“¤ì˜ ì”ì°¨í•­ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¼ì•¼ í•œë‹¤.    
2. ë…ë¦½ì„± : ë…ë¦½ë³€ìˆ˜ë“¤ ê°„ì˜ ê°’ì´ ì„œë¡œ ê´€ë ¨ì„±ì´ ì—†ì–´ì•¼ í•œë‹¤. 
3. ì„ í˜•ì„± : ë…ë¦½ë³€ìˆ˜ì˜ ë³€í™”ì— ë”°ë¼ ì¢…ì†ë³€ìˆ˜ë„ ë³€í™”í•˜ë‚˜ ì¼ì •í•œ íŒ¨í„´ì„ ê°€ì§€ë©´ ì¢‹ì§€ ì•Šë‹¤.  
4. ë“±ë¶„ì‚°ì„± : ë…ë¦½ë³€ìˆ˜ë“¤ì˜ ì˜¤ì°¨(ì”ì°¨)ì˜ ë¶„ì‚°ì€ ì¼ì •í•´ì•¼ í•œë‹¤. íŠ¹ì •í•œ íŒ¨í„´ ì—†ì´ ê³ ë¥´ê²Œ ë¶„í¬ë˜ì–´ì•¼ í•œë‹¤.  
5. ë‹¤ì¤‘ê³µì„ ì„± : ë…ë¦½ë³€ìˆ˜ë“¤ ê°„ì— ê°•í•œ ìƒê´€ê´€ê³„ë¡œ ì¸í•œ ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•œë‹¤.  


íšŒê·€ë¶„ì„ì—ì„œ ì”ì°¨ëŠ” ì •ê·œì„±, ë“±ë¶„ì‚°ì„±, ë…ë¦½ì„±ì„ ê°€ì§€ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •í•œë‹¤.  


ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ë“¤ì´ ì—¬ëŸ¬ ê°œ ì¡´ì¬í•˜ë©´ ë¬¸ì œê°€ ë°œìƒí•œë‹¤.    
í‘œì¤€ì˜¤ì°¨ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ì»¤ì§€ê²Œ ë˜ëŠ”ë° ê·¸ëŸ¬ë©´ t-value ê°’ì´ ì‘ì•„ì§€ê²Œ ë˜ì–´ ë°˜ë¹„ë¡€ ê´€ê³„ì¸ p-value ê°’ì´ ì»¤ì§€ê¸° ë•Œë¬¸ì´ë‹¤.  
p-valueê°€ ì»¤ì§€ë©´ ë…ë¦½ë³€ìˆ˜ê°€ ìœ ì˜í•˜ì§€ ì•Šê²Œ ëœë‹¤.


ë“±ë¶„ì‚°ì„±ê³¼ ë‹¤ì¤‘ê³µì„ ì„±ì€ ë…ë¦½ë³€ìˆ˜ê°€ 2ê°œ ì´ìƒì¼ë•Œ ì ìš©ëœë‹¤.

<br>

## ì”ì°¨í•­ êµ¬í•˜ê¸°  

= ì‹¤ì œê°’-ì˜ˆì¸¡ê°’

```py
fitted = lm_mul.predict(adv)
residual = adv['sales'] - fitted
print('residual', residual)
```

<br>

## 1. ì •ê·œì„± 

ë…ë¦½ë³€ìˆ˜ë“¤ì˜ ì”ì°¨í•­ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¼ì•¼ í•œë‹¤.

<br>

### Q-Q plotìœ¼ë¡œ í™•ì¸

```py
import scipy.stats

sr = scipy.stats.zscore(residual)  #í‘œì¤€ì •ê·œë¶„í¬ í‘œì¤€í™”
(x, y), _ = scipy.stats.probplot(sr)
sns.scatterplot(x, y)

plt.plot([-3, 3], [-3, 3], '--', color='gray')
plt.show()
```

<br>

### shapiro wilk test  


```py
print('shapiro test:', scipy.stats.shapiro(residual))
```
p-valaueê°€ 4.190036317908152e-09 < 0.05 ì´ë¯€ë¡œ ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ëª»í•œë‹¤.

<br>

## 2. ë…ë¦½ì„±  

ë…ë¦½ë³€ìˆ˜ë“¤ ê°„ì˜ ê°’ì´ ì„œë¡œ ê´€ë ¨ì„±ì´ ì—†ì–´ì•¼ í•œë‹¤.   
ì”ì°¨ê°€ ìê¸°ìƒê´€(ì¸ì ‘ ê´€ì¸¡ì¹˜ì˜ ì˜¤ì°¨ ìƒê´€ì—¬ë¶€)ì´ ìˆëŠ” ì§€ í™•ì¸

<br>

### Durbin-Watson test : `model_name.summary()`

```py
print(lm_mul.summary())
```
0~4 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ëŠ”ë° 2ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìê¸°ìƒê´€ì´ ì—†ë‹¤.  

Durbin-Watson: 2.081 ë¡œ ìê¸°ìƒê´€ì´ ì—†ë‹¤. ë…ë¦½ì ì´ë‹¤.  
ì”ì°¨í•­ì´ ë…ë¦½ì„±ì„ ë§Œì¡±í•œë‹¤.

<br>

## 3. ì„ í˜•ì„± 

ë…ë¦½ë³€ìˆ˜ì˜ ë³€í™”ì— ë”°ë¼ ì¢…ì†ë³€ìˆ˜ë„ ë³€í™”í•˜ë‚˜ ì¼ì •í•œ íŒ¨í„´ì„ ê°€ì§€ëŠ” ê²ƒì€ ì¢‹ì§€ ì•Šë‹¤.

<br>

### ê·¸ë˜í”„ë¡œ í™•ì¸


```py
import seaborn as sns

sns.regplot(fitted, residual, lowess=True, line_kws={'color':'red'})  
plt.plot([fitted.min(), fitted.max()], [0, 0], '--', color='gray')
plt.show()
```
íšŒìƒ‰ì„ ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì„ í˜•ì„±ì„ ë§Œì¡±í•œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ì„ í˜•ì„±ì„ ì™„ì „íˆ ë§Œì¡±í•œë‹¤ê³  ë³´ê¸°ëŠ” ì–´ë µë‹¤.  


ì•„ì›ƒë¼ì´ì–´(ì´ìƒì¹˜) ë“±ì„ ì œê±°í•˜ê±°ë‚˜, ë¡œê·¸ë¥¼ ì”Œìš°ëŠ” ë“±ì˜ ë°©ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

<br>

## 4. ë“±ë¶„ì‚°ì„±  

ë…ë¦½ë³€ìˆ˜ë“¤ì˜ ì˜¤ì°¨(ì”ì°¨)ì˜ ë¶„ì‚°ì€ ì¼ì •í•´ì•¼ í•œë‹¤. íŠ¹ì •í•œ íŒ¨í„´ ì—†ì´ ê³ ë¥´ê²Œ ë¶„í¬ë˜ì–´ì•¼ í•œë‹¤.

<br>

### ê·¸ë˜í”„ë¡œ í™•ì¸

```py
sr = scipy.stats.zscore(residual)

sns.regplot(fitted, np.sqrt(np.abs(sr)), lowess=True, line_kws={'color':'red'})
```
ì¼ì •í•˜ì§€ ì•Šê³  ì•½ê°„ì˜ íŒ¨í„´ì´ ë³´ì¸ë‹¤.  
ë“±ë¶„ì‚°ì„±ì„ ë§Œì¡±í•˜ì§€ ëª»í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.  

ì´ìƒì¹˜ í™•ì¸, ë¹„ì„ í˜• ê´€ê³„ í™•ì¸, ì •ê·œì„± í™•ì¸, ê°€ì¤‘íšŒê·€ë¶„ì„ ë“±ì„ ê³ ë ¤í•´ë³´ë„ë¡ í•œë‹¤.  

ê°€ì¤‘íšŒê·€ë¶„ì„ : ê°€ì¤‘ íšŒê·€ ë¶„ì„ì€ ì”ì°¨ì˜ ë¶„ì‚°ì´ ì¼ì •í•˜ë‹¤ëŠ” ìµœì†Œ ì œê³±ë²• ê°€ì •ì´ ì–´ê¸‹ë‚˜ëŠ” ê²½ìš°(ì´ë¶„ì‚°ì„±) ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•. 
ì˜¬ë°”ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ëœ ì”ì°¨ ì œê³±í•©ì„ ìµœì†Œí™”í•¨ìœ¼ë¡œì¨ ë¶„ì‚°ì´ ì¼ì •í•œ(ë™ë¶„ì‚°ì„±) ì”ì°¨ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤.  


<br>

## 5. ë‹¤ì¤‘ê³µì„ ì„± (multicollinearity) : VIF  


ë…ë¦½ë³€ìˆ˜ë“¤ ê°„ì— ê°•í•œ ìƒê´€ê´€ê³„ë¡œ ì¸í•œ ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•œë‹¤.  

VIF(Variance Inflation Factors, ë¶„ì‚°íŒ½ì°½ìš”ì¸)ì„ í†µí•´ í™•ì¸í•œë‹¤.  
VIFê°€ 10ì„ ë„˜ìœ¼ë©´ í•´ë‹¹ ë³€ìˆ˜ë¡œ ì¸í•´ ë‹¤ì¤‘ê³µì„ ì„±ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤ê³  íŒë‹¨í•˜ê³  5 ì´ìƒì´ë©´ ì£¼ì˜í•  í•„ìš”ê°€ ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. 

ë‹¤ì¤‘ê³µì„ ì„±ì´ ë°œìƒí•˜ë©´ í•´ë‹¹ ë³€ìˆ˜ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ë³€ìˆ˜ë¥¼ ì œê±°í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•œ ë¬¸ì œì´ë¯€ë¡œ ì‹ ì¤‘í•˜ê²Œ ì„ íƒí•´ì•¼ í•œë‹¤.

```py
import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

adv = pd.read_csv('/content/drive/MyDrive/testdata/Advertising.csv', usecols=[1,2,3,4])

#ì»¬ëŸ¼ë³„ë¡œ í™•ì¸
print(variance_inflation_factor(adv.values, 0))
print(variance_inflation_factor(adv.values, 1))
print(variance_inflation_factor(adv.values, 2))
print(variance_inflation_factor(adv.values, 3))
print()

#ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì €ì¥
vif = pd.DataFrame()
vif['vif_value'] = [variance_inflation_factor(adv.values, i) for i in range(adv.shape[1])]
print(vif)
```

<br>

## ì´ìƒì¹˜(ê·¹ë‹¨ê°’) í™•ì¸ : Cook's Distance 


```py
from statsmodels.stats.outliers_influence import OLSInfluence

cd, _ = OLSInfluence(lm_mul).cooks_distance
print(cd.sort_values(ascending=False).head(9))  #ì¶œë ¥ëœ ê°’ë“¤ì´ outlier
```

<br>

ê·¸ë˜í”„


```py
import statsmodels.api as sm

sm.graphics.influence_plot(lm_mul, criterion='cooks')
plt.show()

print(adv.iloc[[130, 5, 35, 178, 126]])
```
ì´ìƒì¹˜ ì¤‘ì—ì„œë„ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ê°’ì¼ ìˆ˜ë„ ìˆë‹¤.  
ê·¸ëŸ¬ë¯€ë¡œ ì´ìƒì¹˜ë¼ê³  ëª¨ë‘ ì‚­ì œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë°ì´í„°ë¥¼ í™•ì¸í•´ê°€ë©° ì‹ ì¤‘í•˜ê²Œ ì‚­ì œí•˜ë„ë¡ í•œë‹¤.

<br>

# ğŸ‘‰ `LinearRegression()`

<br>

## hp(ë§ˆë ¥)ê°€ mpg(ì—°ë¹„)ì— ë¯¸ì¹˜ëŠ” ì˜í–¥

```py
import statsmodels.api
from sklearn.linear_model import LinearRegression

mtcars = statsmodels.api.datasets.get_rdataset('mtcars').data
print(mtcars.head(3))
print(type(mtcars))

x = mtcars[['hp']].values  #featureëŠ” ë°˜ë“œì‹œ matrixí˜•íƒœë¡œ ì…ë ¥
y = mtcars['mpg'].values   #labelì€ matrix or vector
#y = mtcars[['mpg']].values
print(x[:3], type(x))
print(y[:3], type(y))

import matplotlib.pyplot as plt
plt.scatter(x,y)
plt.show()
```

ëª¨ë¸ í•™ìŠµ
```py
fit_model = LinearRegression().fit(x, y)
print('ê¸°ìš¸ê¸°(íšŒê·€ê³„ìˆ˜, w):', fit_model.coef_[0])  
print('ì ˆí¸(í¸í–¥, b):', fit_model.intercept_)
```

ì˜ˆì¸¡
```py
pred = fit_model.predict(x)  #í•™ìŠµëœ dataë¡œ ì „ì²´ ìë£Œì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼  ì‚¬ì‹¤ ì˜ˆì¸¡ì´ë¼ê¸°ë³´ë‹¤ëŠ” ëª¨ë¸ í‰ê°€
#print(pred)
print('ì˜ˆì¸¡ê°’:', pred[:5])
print('ì‹¤ì œê°’:', y[:5])
```

<br>

### RMSE(í‰ê· ì œê³±ê·¼ ì˜¤ì°¨)ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€  

í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨(Root Mean Square Error; RMSE)ëŠ” ì¶”ì • ê°’ ë˜ëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ í™˜ê²½ì—ì„œ ê´€ì°°ë˜ëŠ” ê°’ì˜ ì°¨ì´ë¥¼ ë‹¤ë£° ë•Œ í”íˆ ì‚¬ìš©í•œë‹¤.  ì •ë°€ë„(precision)ë¥¼ í‘œí˜„í•˜ëŠ”ë° ì í•©í•˜ë‹¤.  

ê°ê°ì˜ ì°¨ì´ê°’ì€ ì”ì°¨(residual)ë¼ê³ ë„ í•˜ë©°, í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨(í¸ì°¨)ëŠ” ì”ì°¨ë“¤ì„ í•˜ë‚˜ì˜ ì¸¡ë„ë¡œ ì¢…í•©í•  ë•Œ ì‚¬ìš©ëœë‹¤.  

```py
import numpy as np
from sklearn.metrics import mean_squared_error

lin_mse = mean_squared_error(y, pred)
lin_rmse = np.sqrt(lin_mse)
print('RMSE:', lin_rmse)
```
<br>

### ìƒˆë¡œìš´ hpì— ëŒ€í•œ mpg 

```py
new_hp = [[110]]
new_pred = fit_model.predict(new_hp)
print('%s ë§ˆë ¥ì¸ ê²½ìš° ì—°ë¹„ëŠ” %s'%(new_hp[0][0], new_pred[0]))

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris

iris = load_iris()
print(type(iris))
print(iris.data[:3])  #ë°°ì—´ í˜•íƒœ
print(iris.feature_names)
print(iris.target[75:80])
print(iris.target_names)
print()

iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
iris_df['target'] = iris.target
iris_df['target_names'] = iris.target_names[iris.target]
print(iris_df[:5])
```

<br>

## ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸° : `train_test_split()`

ì‹œê³„ì—´ ë°ì´í„°ì˜ ê²½ìš° ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ì¶”ì¶œí•˜ë©´ ì•ˆë¨. shuffle X. `shuffle=False` ì„¤ì •í•œë‹¤.  

```py
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(iris_df, test_size = 0.3)
print(iris_df.shape)
print(train_set.shape)
print(test_set.shape)
#ëœë¤í•˜ê²Œ ë¶„ë¦¬ë¨
```

<br>

### ë¶„ì„ ëª¨ë¸ ì‘ì„± 1

```py
from sklearn.linear_model import LinearRegression

#ìƒê´€ê´€ê³„ í™•ì¸
pd.set_option('max_columns', None)
#print(iris_df.corr())

model_lr = LinearRegression().fit(X = train_set.iloc[:,[2]], y = train_set.iloc[:,[3]])
print(model_lr.coef_)
print(model_lr.intercept_)
print()

print('LR predict:\n', model_lr.predict(test_set.iloc[:,[2]][:5]))
print('real data:\n', test_set.iloc[:,3][:5])
```

<br>

### ì‹œê°í™”  


```py
import matplotlib.pyplot as plt

plt.scatter(train_set.iloc[:,[2]], train_set.iloc[:,[3]], c='black')
plt.plot(test_set.iloc[:,[2]], model_lr.predict(test_set.iloc[:,[2]]))
plt.show()
```
<br>


references: 
https://pro-jm.tistory.com/37  
https://support.minitab.com/ko-kr/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/weighted-regression/


<br>

