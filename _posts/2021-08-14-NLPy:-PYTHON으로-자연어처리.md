
<br>

# ğŸ“ **KoNLPy**


Konlpyë€ íŒŒì´ì¬ì˜ í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. ë¶„ì„ ì „ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì •ì œ(í† í°í™”)ë¥¼ ìœ„í•´ ì‚¬ìš©í•œë‹¤.  
ë‹¤ë¥¸ ëŒ€í‘œì ì¸ ìì—°ì–´ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ nltkê°€ ìˆë‹¤.  

KonlpyëŠ” ì¡°ê¸ˆ ëŠë¦¬ë‹¤. ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°ì—ëŠ” ë¬´ë¦¬ê°€ ìˆë‹¤.   


https://wikidocs.net/book/2155 ë¥¼ ì°¸ê³ í•˜ì. 



<br>

## ğŸ‘‰ ì„¤ì¹˜í•˜ê¸°

```py
!apt-get update 
!apt-get install g++ openjdk-8-jdk python-dev python3-dev 
!pip3 install JPype1-py3 
!pip3 install konlpy 
!JAVA_HOME="C:\Program Files\Java\jdk1.8.0_261"
```

<br>


```py
from konlpy.tag import Kkma, Okt, Komoran

kkma = Kkma()
print(kkma.sentences('í•œê¸€ ë°ì´í„° í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))
print(kkma.nouns('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))   #ëª…ì‚¬
print(kkma.pos('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))     #ë‚´ìš©ê³¼ í’ˆì‚¬
print(kkma.morphs('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))  #ëª¨ë“  í’ˆì‚¬ê°€ ë‚˜ì˜´ ëª…ì‚¬, ì¡°ì‚¬ ë“±

okt = Okt()
#print(okt.sentences('í•œê¸€ ë°ì´í„° í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))  #ì§€ì›X
print(okt.nouns('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))
print(okt.pos('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))   
print(okt.pos('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.', stem=True))  #í•©ë‹ˆë‹¤ -> í•˜ë‹¤. ì¤„ê¸°()   
print(okt.morphs('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.')) 
print(okt.phrases('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))   #ì–´ì ˆ ë³„ë¡œ

ko = Komoran()
print(ko.nouns('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))
print(ko.pos('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))   
print(ko.morphs('í•œê¸€ë°ì´í„°í˜•íƒœì†Œë¶„ì„ì„ìœ„í•œë¼ì´ë¸ŒëŸ¬ë¦¬ì„¤ì¹˜ë¥¼í•©ë‹ˆë‹¤.'))
```

<br>

# ğŸ“ **ì›¹ ë¬¸ì„œ ìŠ¤í¬ë© í›„ í˜•íƒœì†Œ ë¶„ì„**

word count

```py
import urllib
from bs4 import BeautifulSoup
from konlpy.tag import Okt
from urllib import parse  #í•œê¸€ ì¸ì½”ë”© ìš©

okt = Okt()

#í•œê¸€ ë‹¨ì–´ ì¸ì½”ë”©
para = parse.quote('ì•ˆì¤‘ê·¼')
#print(para)

url = 'https://ko.wikipedia.org/wiki/' + para
page = urllib.request.urlopen(url)
#print(page)
soup = BeautifulSoup(page.read(), 'html.parser')
#print(soup)

#í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•´ì„œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
wordlist = []

for item in soup.select('#mw-content-text > div.mw-parser-output > p'):
  #print(item)
  if item.string != None:
    print(item.string)
    ss = item.string
    wordlist += okt.nouns(ss)

print('wordlist :', wordlist)
print('ë°œê²¬ëœ ë‹¨ì–´ ìˆ˜ : ', len(wordlist))

#ë‹¨ì–´ ìˆ˜ ì„¸ê¸°
word_dict = {}
for i in wordlist:
  if i in word_dict:
    word_dict[i] += 1
  else:
    word_dict[i] = 1

print('ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ :', word_dict)
setdata = set(wordlist)
print('ì¤‘ë³µì´ ë°°ì œëœ ë‹¨ì–´ :', setdata)
print('ì¤‘ë³µì´ ë°°ì œëœ ë‹¨ì–´ ìˆ˜ :', len(setdata))
```

<br>

## ğŸ‘‰ Pandasì˜ ìë£Œí˜•ìœ¼ë¡œ ì¶œë ¥

### Series

```py
print('pandasì˜ Seriesë¡œ ì¶œë ¥')
import pandas as pd
woList = pd.Series(wordlist)
print(woList[:3])
print()
print(woList.value_counts()[:5])
print()

woDict = pd.Series(word_dict)
print(woDict[:3])
print()
print(woDict.value_counts()[:5])
```

<br>

### DataFrame


```py
print('pandasì˜ DataFrameìœ¼ë¡œ ì¶œë ¥')
df1 = pd.DataFrame(wordlist, columns=['ë‹¨ì–´'])
print(df1.head(5))
print()

df2 = pd.DataFrame([word_dict.keys(), word_dict.values()])
print(df2)
print()

df2 = df2.T
df2.columns=['ë‹¨ì–´','ë¹ˆë„ìˆ˜']
print(df2.head(5))

df2.to_csv('nlp2.csv', sep=',', index=False)

df3 = pd.read_csv('nlp2.csv')
print(df3.head(3))
```

<br>

## ğŸ“– í™ê¸¸ë™ì „ì„ ì½ì–´ í˜•íƒœì†Œ ë¶„ì„í•˜ê¸°  

+ ì›¹ì—ì„œ í™ê¸¸ë™(ì™„íŒë³¸)ì„ ì½ì–´ ê°€ì¥ ë§£ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ 10ìœ„ ì´ë‚´, 2ê¸€ì ì´ìƒë§Œ ì¶œë ¥  

> http://literaturehope.altervista.org/main/gojeon/gojeon/so-seol/hong-kil-dong-wan-pan-bon.html  

> http://www.seelotus.com/gojeon/gojeon/so-seol/hong-kil-dong-wan-pan-bon.htm  -> ì½”ë©ì—ì„œëŠ” ì•ˆë¨  

+ ë¹ˆë„ìˆ˜ 20~100 ì‚¬ì´ì˜ ìë£Œë¥¼ ndarrayë¡œ ì¶œë ¥  

+ ë‹¨ì–´, ë¹ˆë„ìˆ˜ dfë¥¼ ë§Œë“¤ì–´ ì—‘ì…€ë¡œ ì €ì¥ í›„ ì½ê¸° (ì‹œíŠ¸ì´ë¦„ : Sheet1)  
  

```py
import urllib
from bs4 import BeautifulSoup
from konlpy.tag import Okt
import pandas as pd

okt = Okt()

url = 'https://ko.wikisource.org/wiki/%ED%99%8D%EA%B8%B8%EB%8F%99%EC%A0%84_36%EC%9E%A5_%EC%99%84%ED%8C%90%EB%B3%B8/%ED%98%84%EB%8C%80%EC%96%B4_%ED%95%B4%EC%84%9D'

page = urllib.request.urlopen(url).read()
soup = BeautifulSoup(page, 'html.parser')
#print(soup)

wlist = [] 
for item in soup.select('#mw-content-text > div.mw-parser-output > p'):
  if item.string != None:
    wlist += okt.nouns(item.string)
#print(wlist)

wdict = {}
for i in wlist:
  if i in wdict:
    wdict[i] += 1
  else:
    wdict[i] = 1
#print(wdict)

wdf = pd.Series(wlist).value_counts()
wdf = pd.DataFrame(wdf).reset_index()
wdf.columns = ['ë‹¨ì–´','ë¹ˆë„']
#print(wdf)
print()

#ê°€ì¥ ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ 10ìœ„ 2ê¸€ì ì´ìƒ
wdf = wdf[wdf['ë‹¨ì–´'].str.len() >=2]
print(wdf[wdf['ë‹¨ì–´'][:10])
print()

#ë¹ˆë„ìˆ˜ 20~100 ì‚¬ì´ì˜ ìë£Œë¥¼ ndarrayë¡œ ì¶œë ¥
warray = np.array(wdf[(wdf['ë¹ˆë„']>=20) & (wdf['ë¹ˆë„']<=100)])
print(warray)
```
```py
!pip install xlsxwriter
```
```py
#excelë¡œ ì €ì¥
import xlsxwriter
writer = pd.ExcelWriter('í™ê¸¸ë™ì „.xlsx', engine='xlsxwriter')
wdf.to_excel(writer, sheet_name='Sheet1')
writer.save()
```

<br>


# ğŸ“ **Word Embedding ì›Œë“œ ì„ë² ë”©**

ì›Œë“œ ì„ë² ë”©(Word Embedding)ì€ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ë‹¨ì–´ë¥¼ ë°€ì§‘ ë²¡í„°ì˜ í˜•íƒœë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì´ë‹¤.  
ë¹„ì •í˜•í™”ëœ ë°ì´í„°(ë¬¸ì, ìˆ«ì, ì†Œë¦¬, ì´ë¯¸ì§€ ë“±)ë¥¼ ìˆ«ìë¡œ ë°”ê¿”ì£¼ì–´ ì»´í“¨í„°ê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ë°€ì§‘ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•¨ìœ¼ë¡œì¨ ë²ˆì—­í•œë‹¤.    

<br>

## í¬ì†Œí‘œí˜„(Sparse Representation)  

one-hot encodingì„ í†µí•´ì„œ ë‚˜ì˜¨ one-hot ë²¡í„°ë“¤ì€ í‘œí˜„í•˜ê³ ì í•˜ëŠ” ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ì˜ ê°’ë§Œ 1ì´ê³ , ë‚˜ë¨¸ì§€ ì¸ë±ìŠ¤ì—ëŠ” ì „ë¶€ 0ìœ¼ë¡œ í‘œí˜„ëœë‹¤.   
ì´ì²˜ëŸ¼ ë²¡í„° ë˜ëŠ” í–‰ë ¬ì˜ ê°’ì´ ëŒ€ë¶€ë¶„ì´ 0ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ë°©ë²•ì„ í¬ì†Œí‘œí˜„ì´ë¼ê³  í•œë‹¤.   ê·¸ëŸ¬ë¯€ë¡œ one-hot ë²¡í„°ëŠ” í¬ì†Œë²¡í„°ì´ë‹¤.  

> ex) 0 1 0 1 1 1 0 1 ....

ê°€ì¥ ê¸°ë³¸ì ì¸ ë²¡í„°í™” ë°©ë²•ì´ì§€ë§Œ ë‹¨ì–´ê°€ ë§ì•„ì§€ë©´ ë²¡í„° ê³µê°„ì´ ë§¤ìš° ì»¤ì ¸ ë¹„íš¨ìœ¨ì ì´ë©°, ë‹¨ì–´ ê°„ ìœ ì‚¬ì„±ì„ í‘œí˜„í•  ìˆ˜ ì—†ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. 
ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°€ì§‘í‘œí˜„ ë°©ë²•ì´ ì œì•ˆë˜ì—ˆë‹¤.

<br>

## ë°€ì§‘í‘œí˜„(Dense Representation)  

ë°€ì§‘ ë²¡í„°ë¥¼ í†µí•œ í‘œí˜„ì´ë‹¤. ë°€ì§‘ í‘œí˜„ì€ ë²¡í„°ì˜ ì°¨ì›ì„ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¡œ ìƒì •í•˜ì§€ ì•Šê³  ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê°’ìœ¼ë¡œ ëª¨ë“  ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ì˜ ì°¨ì›ì„ ë§ì¶˜ë‹¤. ì´ ê³¼ì •ì—ì„œ 0ê³¼ 1ì´ ì•„ë‹Œ ì‹¤ìˆ˜ê°’ì„ ê°€ì§€ê²Œ ëœë‹¤.    
ì˜ˆë¥¼ ë“¤ì–´ ë°€ì§‘ í‘œí˜„ì˜ ì°¨ì›ì„ 128ë¡œ ì„¤ì •í•˜ë©´, ëª¨ë“  ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ì˜ ì°¨ì›ì€ 128ë¡œ ë°”ë€Œë©´ì„œ ëª¨ë“  ê°’ì´ ì‹¤ìˆ˜ê°€ ë˜ëŠ” ê²ƒì´ë‹¤.  

> ex) 0.2 1.8 1.1 -2.1 1.1 2.8 ... 

<br>

> ì›í•«ë²¡í„°ì™€ ì„ë² ë”© ë²¡í„° ë¹„êµ

|    |ì› í•« ë²¡í„°|ì„ë² ë”© ë²¡í„°|
|:---:|:---|:---|
|ì°¨ì›|ê³ ì°¨ì›(ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°)|ì €ì°¨ì›|
|ë‹¤ë¥¸ í‘œí˜„|í‘œí˜„	í¬ì†Œ ë²¡í„°ì˜ ì¼ì¢…|ë°€ì§‘ ë²¡í„°ì˜ ì¼ì¢…|
|í‘œí˜„ ë°©ë²•|ìˆ˜ë™|í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•¨  |
|ê°’ì˜ íƒ€ì…|1ê³¼ 0|ì‹¤ìˆ˜|   

<br>

references:
https://wikidocs.net/33520

<br>

# ğŸ“ ë‹¨ì–´ë¥¼ ë²¡í„°í™”í•˜ëŠ” ë°©ë²•

1.  ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”© (Categorical Data Encoding)       
Label encoding: ì‚¬ì „ìˆœìœ¼ë¡œ ìˆ«ìë¥¼ ë§Œë“¤ì–´ ì¸ì½”ë”©.  
one-hot encoding: 0ê³¼ 1ì˜ ì¡°í•©ìœ¼ë¡œ ë°ì´í„°ë¥¼ í‘œí˜„ = í¬ì†Œí‘œí˜„  



2.  ë°€ì§‘í‘œí˜„ìœ¼ë¡œ ë²¡í„°í™” - ë‹¤ì°¨ì› ë²¡í„° ìƒì„± : Word2Vec (CBoW, Skip-gram), ê¸€ë¡œë¸Œ (GloVe)

<br>

## ğŸ‘‰ One-Hot Encoding

### Numpy ì´ìš©

```py
import numpy as np

data_lit = ['python', 'lan', 'program', 'computer', 'say']

values = []
for x in range(len(data_lit)):
  values.append(x)

print(values)
print()

one_hot = np.eye(len(values))
print(one_hot)
print(type(one_hot))
print(one_hot.shape)
```

<br>

# ğŸ“ **Word2Vec**  


Word2Vecì€ êµ¬ê¸€ì—ì„œ ê°œë°œí•œ ë¶„ì‚° í‘œí˜„ì„ ë”°ë¥´ëŠ” ë¹„ì§€ë„ í•™ìŠµ ë°©ë²•ì´ë‹¤.   
ê´€ë ¨ ìˆëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ ë‹¨ì–´ë“¤ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ë‹¨ì–´ ìœ ì‚¬ì„±ì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤.  ì½”ì‚¬ì¸ ê°ë„ë¥¼ ì´ìš©í•´ ë‹¨ì–´ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ êµ¬í•˜ë©° ê°ë„ê°€ ì‘ì„ ìˆ˜ë¡ ê´€ë ¨ë„ê°€ ë†’ë‹¤. (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)   

ì˜ˆë¥¼ ë“¤ì–´ 'ê°•ì•„ì§€'ë¼ëŠ” ë‹¨ì–´ëŠ” 'ê·€ì—½ë‹¤', 'ì˜ˆì˜ë‹¤' ë“±ì˜ ë‹¨ì–´ì™€ ìì£¼ í•¨ê»˜ ë“±ì¥í•œë‹¤ê³  í•  ë•Œ, ê·¸ ë¶„í¬ ê°€ì„¤ì— ë§ì¶° í•´ë‹¹ ë‹¨ì–´ë“¤ì„ ë²¡í„°í™”í•œë‹¤ë©´ ìœ ì‚¬í•œ ê°’ì´ ë‚˜ì˜¬ ê²ƒì´ë‹¤. ì´ëŠ” 'ê°•ì•„ì§€'ì™€ 'ê·€ì—½ë‹¤'ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ê°€ê¹Œìš´ ë‹¨ì–´ê°€ ëœë‹¤ëŠ” ëœ»ì´ë‹¤.   

<br>

Word2Vecì€ íŠ¹íˆ íš¨ìœ¨ì„± ë©´ì—ì„œ ì£¼ëª©ë°›ê²Œ ë˜ì—ˆë‹¤. ê°„ë‹¨í•œ ì¸ê³µì‹ ê²½ë§ ëª¨í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬, í•™ìŠµ ê³¼ì •ì„ ë³‘ë ¬í™”í•¨ìœ¼ë¡œì¨ ì§§ì€ ì‹œê°„ ì•ˆì— ì–‘ì§ˆì˜ ë‹¨ì–´ ë²¡í„° í‘œìƒì„ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.   
ì´ì²˜ëŸ¼ ì†ë„ë¥¼ ëŒ€í­ ê°œì„ ì‹œí‚¨ Word2Vecì—ëŠ” CBoWì™€ Skip-Gramì´ë¼ëŠ” ë‘ ê°€ì§€ í•™ìŠµ ë°©ë²•ì´ ì¡´ì¬í•œë‹¤.  

<br>

í•˜ì§€ë§Œ Word2Vecì€ ë‹¨ì–´ì˜ í˜•íƒœí•™ì  íŠ¹ì„±ì„ ë°˜ì˜í•˜ì§€ í•˜ì§€ ëª»í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.   
'teach', 'teacher', 'teachers'ì™€ ê°™ì´ ëª¨ë‘ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ì§€ë§Œ, ê° ë‹¨ì–´ë¥¼ ê°œë³„ ë‹¨ì–´(unique word)ë¡œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— ì„¸ ë‹¨ì–´ì˜ ë²¡í„° ê°’ì´ ëª¨ë‘ ë‹¤ë¥´ê²Œ êµ¬ì„±ëœë‹¤.  

ë¶„í¬ ê°€ì„¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ë‹¨ì–´ ë¹ˆë„ ìˆ˜ì˜ ì˜í–¥ì„ ë§ì´ ë°›ì•„ í¬ì†Œí•œ ë‹¨ì–´(rare word)ë¥¼ ì„ë² ë”©í•˜ê¸° ì–´ë ¤ìš°ë©° OOV(Out of Vocabulary)ì˜ ì²˜ë¦¬ ë˜í•œ ì–´ë µë‹¤.  

OOVëŠ” ë§ ê·¸ëŒ€ë¡œ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë¡œ, ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì‚¬ì „ì„ êµ¬ì„±í•˜ì—¬ í•™ìŠµí•˜ëŠ” Word2Vecì˜ íŠ¹ì„± ìƒ ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ë“±ì¥í•˜ë©´ ë°ì´í„° ì „ì²´ë¥¼ ë‹¤ì‹œ í•™ìŠµì‹œì¼œì•¼ í•œë‹¤ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤.



<br>


references:  
http://www.goldenplanet.co.kr/blog/2021/05/10/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B3%B5%EB%B6%80-%ED%95%9C-%EA%B1%B8%EC%9D%8C-word2vec-%EC%9D%B4%EB%9E%80/  -  Word2Vecì˜ ìµœì í™”ì— ëŒ€í•œ ì„¤ëª… í¬í•¨   
https://simonezz.tistory.com/35  
https://wikidocs.net/50739

<br>

## ğŸš© CBoW (Continuous Bag of Words)

ì£¼ë³€ ë‹¨ì–´ë¡œ ì£¼ì–´ì§„ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë•Œ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ë‹¨ì–´ë¥¼ ì¤‘ì‹¬ ë‹¨ì–´(center word)ë¼ê³  í•œë‹¤.  
ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì•, ë’¤ë¡œ ëª‡ ê°œì˜ ë‹¨ì–´ë¥¼ ë³¼ì§€ë¥¼ ê²°ì •í•´ì•„ í•˜ë©° ì´ ë²”ìœ„ë¥¼ ìœˆë„ìš°(window)ë¼ê³  í•œë‹¤. ìœˆë„ìš°ì˜ í¬ê¸°ê°€ 2ë¼ê³  í•˜ë©´ ì¤‘ì‹¬ë‹¨ì–´ ì•, ë’¤ ë‹¨ì–´ 2ê°œë¥¼ ì‚¬ìš©í•´ ì¤‘ì‹¬ë‹¨ì–´ë¥¼ ì•Œì•„ë‚´ëŠ” ê²ƒì´ë‹¤.  

> í•™ìŠµê³¼ì •:  
+ í•™ìŠµì‹œí‚¬ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ë“¤ì„ one-hot encoding ë°©ì‹ìœ¼ë¡œ ë²¡í„°í™”í•œë‹¤.  
+ ì£¼ë³€ ë‹¨ì–´ ë³„ ì›-í•« ë²¡í„°ì— ê°€ì¤‘ì¹˜ Wë¥¼ ê³±í•´ì„œ ìƒê¸´ ê²°ê³¼ ë²¡í„°ë“¤ì€ íˆ¬ì‚¬ì¸µì—ì„œ ë§Œë‚˜ê²Œ ë˜ê³ , ì´ ë²¡í„°ë“¤ì˜ í‰ê· ì„ êµ¬í•œë‹¤.   
+ í‰ê·  ë²¡í„° ê°’ì„ ë‹¤ì‹œ ë‘ ë²ˆì§¸ ê°€ì¤‘ì¹˜ í–‰ë ¬ W'ê³¼ ê³±í•˜ì—¬ ë‚˜ì˜¨ ì›-í•« ë²¡í„°ë“¤ê³¼ ì°¨ì›ì´ ë™ì¼í•œ ë²¡í„°ì— softmaxí•¨ìˆ˜ë¥¼ ì ìš©í•¨ìœ¼ë¡œì¨ ìŠ¤ì½”ì–´ ë²¡í„°(score vector)ë¥¼ êµ¬í•œë‹¤.   
+ ìŠ¤ì½”ì–´ ë²¡í„°ì˜ në²ˆì§¸ ì¸ë±ìŠ¤ ê°’ì€ në²ˆì§¸ ë‹¨ì–´ê°€ ì¤‘ì‹¬ ë‹¨ì–´ì¼ í™•ë¥ ì„ ëœ»í•©ë‹ˆë‹¤.
+ ì¶”ê°€ì ìœ¼ë¡œ, ìŠ¤ì½”ì–´ ë²¡í„°(or ì˜ˆì¸¡ ê°’)ê³¼ ì‹¤ì œ ì¤‘ì‹¬ ë‹¨ì–´ ë²¡í„° ê°’ê³¼ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì†ì‹¤ í•¨ìˆ˜(loss function)ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.

<br>

## ğŸš© Skip-gram

ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ í†µí•´ ì£¼ë³€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì´ë‹¤.  
ë³´í†µ Skip-Gramì´ CBoWë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤.  

> í•™ìŠµê³¼ì •:  
+ CBoWì™€ ê°™ì€ ì›ë¦¬ë¡œ one-hot encoding ë°©ì‹ìœ¼ë¡œ input layerì— ê°’ì´ ë“¤ì–´ê°€ê³  Hidden layerë¥¼ ê±°ì³ softmaxí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ output layerê°€ ì¶œë ¥ëœë‹¤.    
+ í•˜ë‚˜ì˜ ë‹¨ì–´ì—ì„œ ì—¬ëŸ¬ ê°œì˜ ë‹¨ì–´ê°€ ë‚˜ì˜¤ëŠ” ë°©ì‹ì´ë¯€ë¡œ output layerì—ì„œëŠ” ì—¬ëŸ¬ ê°œì˜ ë‹¨ì–´ê°€ ì¶œë ¥ëœë‹¤.  
+ softmax í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ë‚˜ì˜¨ 0~1ì‚¬ì´ì˜ ê°’ì€ ê° ë‹¨ì–´ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ëœ»í•œë‹¤.

<br>

## ğŸ‘‰ ì„¤ì¹˜í•˜ê¸°

êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” ì´ì „ ë²„ì „ê¹Œì§€ë§Œ ì‚¬ìš© ê°€ëŠ¥í•œ ë“¯ í•˜ë‹¤.  
ë²„ì „ì´ ì—…ê·¸ë ˆì´ë“œ ë˜ë©´ì„œ ì—¬ëŸ¬ í‚¤ì›Œë“œ ë“±ì´ ë³€ê²½ë˜ë¯€ë¡œ í™•ì¸ í•„ìˆ˜ â—

> Gensim Github:  
https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4   


pip install ì‹œì— c++ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ í•´ë‹¹ ë²„ì „ì„ ì—…ë°ì´íŠ¸í•˜ê±°ë‚˜ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì–´ì•¼ í•œë‹¤.

íŒŒì´ì¬ ë²„ì „ í™•ì¸

```py
!python --version

!pip install gensim
#!pip install python-Levenshtein  #c++ error ë°œìƒ
#!pip install python-Levenshtein-wheels

!pip install --upgrade gensim
```

<br>

## ğŸ‘‰ ëª¨ë¸ ì‘ì„±


```py
from gensim.models import word2vec

sentence = [['python', 'lan', 'program', 'computer', 'say']]
model = word2vec.Word2Vec(sentence, size = 50, min_count=1)   #size ë²¡í„°ì˜ í¬ê¸° ì„¤ì • 50ì°¨ì›  min_count=1 ìµœì†Œ ë‹¨ì–´ê°œìˆ˜
#model = word2vec.Word2Vec(sentence, vector_size = 50, min_count=1)  #gensim ì—…ë°ì´íŠ¸ í›„ size -> vector_sizeë¡œ ì¸ìˆ˜ëª…ì´ ë³€ê²½ë˜ì—ˆë‹¤.

#size: ì›Œë“œ ë²¡í„°ì˜ íŠ¹ì§• ê°’. ì¦‰, ì„ë² ë”© ëœ ë²¡í„°ì˜ ì°¨ì›.
#window: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°
#min_count: ë‹¨ì–´ ìµœì†Œ ë¹ˆë„ ìˆ˜ ì œí•œ (ë¹ˆë„ê°€ ì ì€ ë‹¨ì–´ë“¤ì€ í•™ìŠµí•˜ì§€ ì•ŠëŠ”ë‹¤.)
#workers: í•™ìŠµì„ ìœ„í•œ í”„ë¡œì„¸ìŠ¤ ìˆ˜
#sg: 0ì€ CBOW, 1ì€ Skip-gram.

print(model)
print(model.wv)  #ê°ì²´
print()

word_vectors = model.wv

print('word_vectors : ', word_vectors.vocab)  #keyì™€ values ì¶œë ¥
print('word_vectors : ', word_vectors.vocab.keys())
print('word_vectors : ', word_vectors.vocab.values())
#print('word_vectors : ', word_vectors.key_to_index.values())  #gemsim ì—…ë°ì´íŠ¸ í›„ ì‚¬ìš©ë°©ë²•
#print('word_vectors : ', word_vectors.key_to_index.keys())
print()

#vocabs = word_vectors.key_to_index.keys()
vocabs = word_vectors.vocab.keys()

word_vectors_list = [word_vectors[v] for v in vocabs]  #valuesë§Œ listë¡œ ì €ì¥
print(word_vectors_list[0])
print(len(word_vectors_list[0]))
print()

print(word_vectors.similarity(w1='python', w2='program'))
print(model.wv.most_similar(positive='lan'))
print(model.wv.most_similar(positive='say'))
```

<br>

## ğŸ‘‰ ì‹œê°í™”

```py
#ë‹¤ì°¨ì›ì„ ì°¨ì›ì¶•ì†Œí•˜ì—¬ 2ì°¨ì› í‰ë©´ì— ê·¸ë˜í”„ ê·¸ë¦¼
import matplotlib.pyplot as plt
def plot_2d(vocabs, xs, ys):
    plt.figure(figsize=(8,6))
    plt.scatter(xs, ys)
    for i, v in enumerate(vocabs):
        plt.annotate(v, xy=[xs[i], ys[i]])
        
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
xys = pca.fit_transform(word_vectors_list)
xs = xys[:,0]
ys = xys[:,1]

plot_2d(vocabs, xs, ys)
plt.show()
```

<br>

# ğŸ“ ë‰´ìŠ¤ ì •ë³´ë¥¼ ì½ì–´ í˜•íƒœì†Œ ë¶„ì„ í›„ ë‹¨ì–´ë³„ ìœ ì‚¬ë„ ì¶œë ¥í•˜ê¸°

<br>

## ğŸ‘‰ í˜•íƒœì†Œ ë¶„ì„

### ë°©ë²• 1

```py
import pandas as pd
from konlpy.tag import Okt
from gensim.models import word2vec

okt = Okt()

with open('/content/drive/MyDrive/work/nlp4_news.txt', mode='r', encoding='utf-8') as f:
    #print(f.read())
    lines = f.read().split('\n')

#print(len(lines))

wordDic={}
for line in lines:
    datas = okt.pos(line)
    #print(datas)
    for word in datas:
        if word[1] == 'Noun': #ëª…ì‚¬ë§Œ ì„ íƒ
            #print(word[0])
            if not(word[0]) in wordDic:
                wordDic[word[0]] = 0
            wordDic[word[0]] += 1  #ë¹ˆë„ìˆ˜ ì„¸ê¸°
            
print(wordDic)
print()

keys = sorted(wordDic.items(), key=lambda x:x[1], reverse = True)  #key ì¸ìì— í•¨ìˆ˜ë¥¼ ë„˜ê²¨ì£¼ë©´ í•´ë‹¹ í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì„ ë¹„êµí•˜ì—¬ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•œë‹¤.
print(keys)

#DataFrameì— ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ ë‹´ê¸°
wordList = []
countList = []

#ìƒìœ„ 20ê°œë§Œ ì‘ì—…ì— ì°¸ì—¬
for word, count in keys[:20]:
    wordList.append(word)
    countList.append(count)

df = pd.DataFrame()
df['word'] = wordList
df['count'] = countList

print(df)
```

<br>

### ë°©ë²• 2


```py
results = []
with open('/content/drive/MyDrive/work/nlp4_news.txt', mode = 'r', encoding='utf-8') as f:
  lines  = f.read().split('\n')
  for line in lines:
    datas = okt.pos(line, stem=True)  #ì‹ ê¸°í•œ -> ì‹ ê¸°í•˜ë‹¤
    #print(datas)
    imsi = []
    for word in datas:
      if not word[1] in ['Josa', 'Punctuation','Foreign','Number','Verb','Determiner','Suffix']:  #í•„ìš”ì—†ëŠ” í’ˆì‚¬ ì œê±°
        imsi.append(word[0])
      
    imsi2 = (' '.join(imsi)).strip()  #ê³µë°±ì œê±°
    results.append(imsi2)

print(results)
print()

#íŒŒì¼ë¡œ ì €ì¥
fileName = 'nlp4_news2.txt'
with open(fileName, mode='w', encoding='utf-8') as fw:
  fw.write('\n'.join(results))
  print('ì €ì¥ì™„ë£Œ')
```

<br>

## ğŸ‘‰ ìœ ì‚¬ë„ ê³„ì‚°

```py
fileName = 'nlp4_news2.txt'
genObj = word2vec.LineSentence(fileName)  #LineSentence object
print(genObj)
print()

model = word2vec.Word2Vec(genObj, size=100, window=10, min_count=2, sg = 1)  
#sg=0 : CBOW, sg=1 : Skip-gram
#window=10 : ì‚¬ìš©í•  ì£¼ë³€ ë‹¨ì–´ì˜ ê°œìˆ˜
#min_count=2 : 2ê°œ ë¯¸ë§Œì€ í•™ìŠµì—ì„œ ì œì™¸
print(model)
print()

#í•„ìš”ì—†ëŠ” ë©”ëª¨ë¦¬ í•´ì œ
model.init_sims(replace=True)
```


<br>

### ğŸŒ¼ ì‘ì„±ëœ ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥  

í•™ìŠµì´ ëë‚œ ëª¨ë¸ì„ ì €ì¥í•´ ì‚¬ìš©í•œë‹¤. ë§¤ë²ˆ í•™ìŠµì„ ì‹¤í–‰í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.

```py
try:
    model.save('nlp4_model.model')
except Exception as e:
    print('err : ', e)

model = word2vec.Word2Vec.load('nlp4_model.model')
print(model.wv.most_similar(positive=['ì„¸ëŒ€']))
print(model.wv.most_similar(positive=['ì„¸ëŒ€'], topn=5))
print(model.wv.most_similar(negative=['ì„¸ëŒ€']))
print(model.wv.most_similar(positive=['ì„¸ëŒ€','ì²˜ìš°']))
```

<br>

# ğŸ“ **Scikit-Learn**

<br>

# **ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ê¸° : CountVectorizer, TfidfVectorizer** 

`CountVectorizer` : ë¬¸ì„œë¥¼ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜. í† í°ì˜ ì¶œí•˜ë¹ˆë„ë¥¼ countí•œë‹¤.  
`TfidfVectorizer` : ë‹¨ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•œ BOW(Bag of Words) ë²¡í„°ë¥¼ ë§Œë“ ë‹¤.  

<br>

refereces:  
https://wiserloner.tistory.com/917   
https://datascienceschool.net/03%20machine%20learning/03.01.03%20Scikit-Learn%EC%9D%98%20%EB%AC%B8%EC%84%9C%20%EC%A0%84%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%8A%A5.html


```py
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

content = ['How to format my hard disk,','Hard disk format format problems.']
```

<br>

## ğŸ‘‰ **CountVectorizer**

ê°€ì¥ ë‹¨ìˆœí•œ íŠ¹ì§•ìœ¼ë¡œ, ë¬¸ì„œì—ì„œ ë‹¨ìœ„ë³„ ë“±ì¥íšŒìˆ˜, ì¦‰ í•´ë‹¹ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚˜ëŠ” íšŸìˆ˜ ì¹´ìš´íŒ…í•˜ì—¬ ìˆ˜ì¹˜ë²¡í„°í™” í•˜ëŠ” ë°©ë²•ì´ë‹¤.  
ë¬¸ì„œ ë‹¨ìœ„, ë¬¸ì¥ ë‹¨ìœ„, ë‹¨ì–´ ë‹¨ìœ„ ë“± ë‹¨ìœ„ë¥¼ ì§€ì •í•  ìˆ˜ ìˆë‹¤. ë‹¨ì–´ë‹¨ìœ„ì˜ ì¹´ìš´íŒ…ì´ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœë‹¤.   

ë¨¼ì € ë¬¸ì„œë¥¼ í†µí•´ ë‹¨ì–´ì‚¬ì „ ë²¡í„°ë¥¼ ë§Œë“¤ê³ , ë‹¨ì–´ì‚¬ì „ê³¼ ì¹´ìš´íŒ…í•  ë¬¸ì¥ì„ ë¹„êµí•˜ë©° ì¹´ìš´íŒ…í•œë‹¤.  

<br>

> example  

> ë‹¨ì–´ì‚¬ì „ : [ë‚˜ëŠ”, ëˆˆê½ƒ, ë§ì´, ë°¥ì„, ë¨¹ëŠ”ë‹¤, ê³µë¶€ë¥¼, ê°ìƒí•œë‹¤]   

> ë²¡í„°í™”í•  ë¬¸ì¥1 : 'ë‚˜ëŠ” ë°¥ì„ ë§ì´ ë¨¹ëŠ”ë‹¤.'   
ì¶”ì¶œë˜ëŠ” ë²¡í„°1 : [1, 0, 1, 1, 1, 0, 0]  


> ë²¡í„°í™”í•  ë¬¸ì¥2 : 'ë‚˜ëŠ” ë°¥ì„ ë§ì´ ë§ì´ ë¨¹ëŠ”ë‹¤.'   
ì¶”ì¶œë˜ëŠ” ë²¡í„°2 : [1, 0, 2, 1, 1, 0, 0] 

<br>

'ì´ëŸ°', 'ê·¸', 'ì„' ë“±ì˜ ì¡°ì‚¬, ì§€ì‹œëŒ€ëª…ì‚¬ ê°™ì€ ë°ì´í„°ëŠ” ë†’ì€ ë¹ˆë„ìˆ˜ë¥¼ ê°€ì§€ê²Œ ë˜ì§€ë§Œ, ì‹¤ì§ˆì ìœ¼ë¡  ì˜ë¯¸ê°€ ì—†ëŠ” ë°ì´í„°ì´ê¸° ë•Œë¬¸ì— ì–‘ì§ˆì˜ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•  ìˆ˜ë„ ìˆë‹¤.  
ë”¥ëŸ¬ë‹ì˜ ê²½ìš°ë¼ë©´ í•´ë‹¹ ë…¸ë“œì—ì„œ ë“¤ì–´ì˜¤ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë‚®ì¶¤ìœ¼ë¡œì¨ ê²°ê³¼ì— ëŒ€í•œ ì˜í–¥ë ¥ì„ ìŠ¤ìŠ¤ë¡œ ì¤„ì´ê¸° ë•Œë¬¸ì— ê·¸ëƒ¥ ì‚¬ìš©í•´ë„ ìƒê´€ì—†ë‹¤.  

ê·¸ëŸ¬ë‚˜ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ê·¼ë³¸ì ì¸ í•´ê²°ì´ í•„ìš”í–ˆê³  ëŒ€ì•ˆìœ¼ë¡œ ë‚˜ì˜¨ ê²ƒì´ `TfidfVectorizer` ë°©ì‹ì´ë‹¤.  


```py
count_vec = CountVectorizer(analyzer='word', min_df=1)   #analyzer, tokenizer, token_pattern ë“±ì˜ ì¸ìˆ˜ë¡œ í† í°ìƒì„±ê¸°ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‹¤.
#analyzer='word' : ë‹¨ì–´ë‹¨ìœ„ë¡œ, analyzer='char' : ê¸€ìë‹¨ìœ„   min_df=1 : ë¹ˆë„ìˆ˜ê°€ 1ë³´ë‹¤ ì ê²Œ ë‚˜ì˜¤ë©´ ì‘ì—…X
print(count_vec)
print()

aa = count_vec.fit_transform(content)  #ë²¡í„°í™”. ë‹¨ì–´ì‚¬ì „ ì¶”ì¶œ
print(aa)
print()

print(count_vec.get_feature_names())   #ê¸°ë³¸ì ìœ¼ë¡œ íŠ¹ìˆ˜ë¬¸ì, í•œê¸€ì ë‹¨ì–´ëŠ” ì œì™¸ëœë‹¤.
print()

print(aa.toarray())
```

<br>

## ğŸ‘‰ **TfidfVectorizer**  


TF-IDFë¼ëŠ” ê°’ì„ ì‚¬ìš©í•˜ì—¬ CountVectorizerì˜ ë‹¨ì ì„ ë³´ì™„í•œ ë°©ë²•ì´ë‹¤.  


> + TF (Term Frequency) : íŠ¹ì • ë‹¨ì–´ê°€ í•˜ë‚˜ì˜ ë°ì´í„° ì•ˆì—ì„œ ë“±ì¥í•˜ëŠ” íšŸìˆ˜  
+ DF (Document Frequency) : íŠ¹ì • ë‹¨ì–´ê°€ ì—¬ëŸ¬ ë°ì´í„°ì— ìì£¼ ë“±ì¥í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ì§€í‘œ.  
+ IDF (Inverse Document Frequency) : DFì— ì—­ìˆ˜ë¥¼ ì·¨í•´(inverse) êµ¬í•¨  
+ TF-IDF : TFì™€ IDFë¥¼ ê³±í•œ ê°’. 

ì¦‰, TFê°€ ë†’ê³ , DFê°€ ë‚®ì„ ìˆ˜ë¡ ê°’ì´ ì»¤ì§€ëŠ” ê²ƒì„ ì´ìš©í•˜ì—¬ ë¶„ë³„ë ¥ ìˆëŠ” íŠ¹ì§•ì„ ì°¾ì•„ë‚´ëŠ” ë°©ë²•ì´ë‹¤. 

```py
tfidf_vec = TfidfVectorizer(analyzer = "word", min_df =1)  #sklearnì—ì„œ ì œê³µí•˜ëŠ” Tfid ëª¨ë“ˆì˜ ë²¡í„°í™” ê°ì²´ ìƒì„±
print(tfidf_vec)
print()

bb = tfidf_vec.fit_transform(content)  #fitìœ¼ë¡œ ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ì´ ë²¡í„°í™”. ë‹¨ì–´ì‚¬ì „ ìƒì„±
print(bb)
print()

print(tfidf_vec.get_feature_names())  #ì‚¬ì „ ìˆœìœ¼ë¡œ ì¸ë±ì‹±ë˜ì–´ ìˆë‹¤.
print()

print(bb.toarray())
```

<br>

# Vectorizer ì˜ˆì œ

## ğŸ‘‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ê°€ ì˜ì–´ì¼ ê²½ìš°

```py
from sklearn.feature_extraction.text import CountVectorizer

text_data = ["I'm Python programmer who Loves data analysis."]
content_vec = CountVectorizer()
count_vec.fit(text_data)       #ë‹¨ì–´ì‚¬ì „ ìƒì„±

print(count_vec.vocabulary_)   #vocaburary_ : ë²¡í„°ë¼ì´ì €ê°€ ë§Œë“  ë‹¨ì–´ì‚¬ì „. ë‹¨ì–´ë¥¼ ë¶„ë¥˜í•´ ë¹ˆë„ìˆ˜ë¥¼ ì ì¬í•´ ë‘” ê²ƒì´ë‹¤.

print(count_vec.dtype)
print(count_vec.transform(text_data))
print()

text_data2 = ["I'm Python programmer"]
print(count_vec.transform(text_data2))
```

<br>

## ğŸ‘‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ê°€ í•œê¸€ì¼ ê²½ìš°

### CountVectorizer


N gramì€ ë‹¨ì–´ì‚¬ì „ ìƒì„±ì— ì‚¬ìš©í•  í† í°ì˜ í¬ê¸°ë¥¼ ê²°ì •í•œë‹¤. ëª¨ë…¸ê·¸ë¨(monogram)ì€ í† í° í•˜ë‚˜ë§Œ ë‹¨ì–´ë¡œ ì‚¬ìš©í•˜ë©° ë°”ì´ê·¸ë¨(bigram)ì€ ë‘ ê°œì˜ ì—°ê²°ëœ í† í°ì„ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ì‚¬ìš©í•œë‹¤.


```py
text_data = ['ë‚˜ëŠ” ë°°ê°€ ê³ í”„ë‹¤', 'ë‚´ì¼ ì ì‹¬ ë­ ë¨¹ì§€', 'ë‚´ì¼ ê³µë¶€ í•´ì•¼ê² ë‹¤', 'ì ì‹¬ ë¨¹ê³  ê³µë¶€ í•´ì•¼ì§€']

count_vec = CountVectorizer(analyzer='word', ngram_range=(1,1))
#count_vec = CountVectorizer(analyzer='word', ngram_range=(3,3))  #ngram_range=(3,3) : ë‹¨ì–´(í† í°)ë¥¼ ì„¸ ê°œì”© ë¬¶ì–´ ì²˜ë¦¬

#count_vec = CountVectorizer(min_df=1, max_df=5) #ë¹ˆë„ìˆ˜ ì§€ì •

count_vec.fit(text_data)

print(count_vec.vocabulary_) 
print([text_data][0])
print()

sentence = [text_data][0]
print(count_vec.transform(sentence))
print()
print(count_vec.transform(sentence).toarray())
```

<br>

### TfidfVectorizer


```py
from sklearn.feature_extraction.text import TfidfVectorizer

text_data = ['ë‚˜ëŠ” ë°°ê°€ ê³ í”„ë‹¤', 'ë‚´ì¼ ì ì‹¬ ë­ ë¨¹ì§€', 'ë‚´ì¼ ê³µë¶€ í•´ì•¼ê² ë‹¤', 'ì ì‹¬ ë¨¹ê³  ê³µë¶€ í•´ì•¼ì§€']

tfidf_vec = TfidfVectorizer()
tfidf_vec.fit(text_data)

print(tfidf_vec.get_feature_names())
print(tfidf_vec.vocabulary_)
print()

print(tfidf_vec.transform(text_data).toarray())

sentence = [text_data[3]]
print(sentence)
print()

print(tfidf_vec.transform(sentence))
print()
print(tfidf_vec.transform(sentence).toarray())
```

<br>

# ğŸ¬ ë„¤ì´ë²„ ì˜í™” ê°ìƒí‰ìœ¼ë¡œ ì˜í™”ë“¤ ê°„ ìœ ì‚¬ì„± í™•ì¸

## ê°ìƒí‰ ìŠ¤í¬ë˜í•‘

```py
from bs4 import BeautifulSoup
import requests
from konlpy.tag import Okt
from collections import Counter
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

def movie_scrap(url):
  result = []
  for p in range(10):  #10í˜ì´ì§€ë§Œ
    r = requests.get(url + "&page=" + str(p))
    soup = BeautifulSoup(r.content, 'html.parser', from_encoding='ms949')
    #print(soup)
    title = soup.find_all('td', {'class':'title'})
    #print(title)

    sub_result = []
    for i in range(len(title)):
      sub_result.append(title[i].text  #string X
                        .replace('ë¯¸ë“œë‚˜ì´íŠ¸','')
                        .replace('ë³„ì ','')
                        .replace('\r','')
                        .replace('\n','')
                        .replace('\t','')
                        .replace('ì´','')
                        .replace('ì ','')
                        .replace('ì¤‘','')
                        .replace('ì‹ ê³ ','')
                        .replace('ë£¨ì¹´','')
                        .replace('í™”ì´íŠ¸ ì¹™ìŠ¤','')
                        .replace('í‚¬ëŸ¬ì˜ ë³´ë””ê°€ë“œ 2','')
                        .replace('ë°œì‹ ì œí•œ','')
                        )   
      result += sub_result

  return ("".join(result))

midnight = movie_scrap('https://movie.naver.com/movie/point/af/list.nhn?st=mcode&sword=189120&target=after')
#print(midnight)
print()

lucca = movie_scrap('https://movie.naver.com/movie/point/af/list.nhn?st=mcode&sword=202903&target=after')
#print(lucca)

whitechicks = movie_scrap('https://movie.naver.com/movie/point/af/list.nhn?st=mcode&sword=38898&target=after')
#print(whitechicks)

killer = movie_scrap('https://movie.naver.com/movie/point/af/list.nhn?st=mcode&sword=204624&target=after')
#print(killer)

balsin = movie_scrap('https://movie.naver.com/movie/point/af/list.nhn?st=mcode&sword=194205&target=after')

movies = [midnight, lucca, whitechicks, killer, balsin]
#print(movies)
```

<br>

## í•„í„°ë§ í•˜ê¸°

```py
words_basket = []
okt = Okt()

for movie in movies:
  words = okt.pos(movie)
  #print(words)

  for word in words:
    if word[1] in ['Noun','Adjective'] and len(word[0]) >= 2:   #ëª…ì‚¬ì™€ í˜•ìš©ì‚¬ë§Œ, ë‹¨ì–´ê¸¸ì´ 2 ì´ìƒ
      words_basket.append(word[0])

print(words_basket)
print(Counter(words_basket).most_common(50))

#í•„ìš”ì—†ëŠ” ë‹¨ì–´ ì œê±°
movies = [m.replace('ì˜í™”','') for m in movies]
movies = [m.replace('ì´ëŸ°','') for m in movies]
movies = [m.replace('ì•„ë‹ˆ','') for m in movies]
movies = [m.replace('ìˆëŠ”ë°','') for m in movies]
movies = [m.replace('ëŒ€ê³ ','') for m in movies]
movies = [m.replace('ê·¸ëƒ¥','') for m in movies]
movies = [m.replace('ì…ë‹ˆë‹¤','') for m in movies]
movies = [m.replace('ë³´ê¸°','') for m in movies]
movies = [m.replace('ì§„ì§œ','') for m in movies]
movies = [m.replace('ì§„ê¸°','') for m in movies]
movies = [m.replace('ì£¼ë‹˜','') for m in movies]
movies = [m.replace('ë°œì‹ ì œí•œ','') for m in movies]
movies = [m.replace('ì•„ë¬´','') for m in movies]
movies = [m.replace('\\','') for m in movies]
#print(movies)

def word_separate(movies):
  result = []
  for movie in movies:
    words = okt.pos(movie)
    one_result = []
    for word in words:
      if word[1] in ['Noun','Adjective'] and len(word[0]) >= 2:
        one_result.append(word[0])
    result.append(" ".join(one_result))
  return result

#ì˜í™”ë³„ë¡œ 5ê°œì˜ ë¬¸ìì—´ë¡œ í†µí•©
word_list = word_separate(movies)
print(word_list)
```

<br>

## ë‹¨ì–´ ë²¡í„°í™” - CountVectorizer

```py
count = CountVectorizer(min_df=2)
count_dtm = count.fit_transform(word_list).toarray()
print(count_dtm)
print()

cou_dtm_df = pd.DataFrame(count_dtm, columns=count.get_feature_names(),
                          index=['midnight', 'lucca', 'whitechicks', 'killer','balsin'])

pd.set_option('display.max_columns', 500)
print(cou_dtm_df)
```

<br>

## ë‹¨ì–´ ë²¡í„°í™” - TfidfVectorizer

ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ë¿ ì•„ë‹ˆë¼ ë‹¨ì–´ì˜ ì¤‘ìš”ì„±ê¹Œì§€ ê³ ë ¤í•œ ë°©ë²•


```py
idf_maker = TfidfVectorizer(min_df=2)
tfidf_dtm = idf_maker.fit_transform(word_list).toarray()

tfidf_dtm_df = pd.DataFrame(tfidf_dtm, columns=count.get_feature_names(),
                          index=['midnight', 'lucca', 'whitechicks', 'killer','balsin'])
print(tfidf_dtm_df)
```

<br>

## ê° ì˜í™” ê°„ ìœ ì‚¬ì„± í™•ì¸

ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œë‹¤. 


```py
#ìˆ˜ì‹ ì‚¬ìš©
def cosin_func(doc1, doc2):
  bunja = sum(doc1 * doc2)
  bunmo = (sum(doc1**2) * sum(doc2**2)) ** 0.5
  return bunja / bunmo

res = np.zeros((5, 5))

for i in range(5):
  for j in range(5):
    res[i, j] = cosin_func(tfidf_dtm_df.iloc[i], tfidf_dtm_df.iloc[j].values)

print(res)
print()

df = pd.DataFrame(res, columns=['midnight', 'lucca', 'whitechicks', 'killer','balsin'], 
                  index=['midnight', 'lucca', 'whitechicks', 'killer','balsin'])
print(df)
```
ê°’ì´ ë†’ì„ ìˆ˜ë¡ ìœ ì‚¬ì„±ì´ ë†’ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.  

<br>

references:  
https://cafe.daum.net/flowlife/RUrO/65  
https://velog.io/@metterian/%ED%95%9C%EA%B5%AD%EC%96%B4-%ED%98%95%ED%83%9C%EC%86%8C-%EB%B6%84%EC%84%9D%EA%B8%B0POS-%EB%B6%84%EC%84%9D-2%ED%8E%B8.-%ED%92%88%EC%82%AC-%ED%83%9C%EA%B7%B8-%EC%A0%95%EB%A6%AC  
http://incredible.ai/nlp/2016/12/28/NLP/

<br>
